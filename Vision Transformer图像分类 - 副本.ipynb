{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Vision Transformer图像分类\n",
    "\n",
    "[![下载Notebook](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/resource/_static/logo_notebook.svg)](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/r2.3/tutorials/application/zh_cn/cv/mindspore_vit.ipynb)&emsp;[![下载样例代码](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/resource/_static/logo_download_code.svg)](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/r2.3/tutorials/application/zh_cn/cv/mindspore_vit.py)&emsp;[![查看源文件](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/resource/_static/logo_source.svg)](https://gitee.com/mindspore/docs/blob/r2.3/tutorials/application/source_zh_cn/cv/vit.ipynb)\n",
    "\n",
    "感谢[ZOMI酱](https://gitee.com/sanjaychan)对本文的贡献。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Vision Transformer（ViT）简介\n",
    "\n",
    "近些年，随着基于自注意（Self-Attention）结构的模型的发展，特别是Transformer模型的提出，极大地促进了自然语言处理模型的发展。由于Transformer的计算效率和可扩展性，它已经能够训练具有超过100B参数的空前规模的模型。\n",
    "\n",
    "ViT则是自然语言处理和计算机视觉两个领域的融合结晶。在不依赖卷积操作的情况下，依然可以在图像分类任务上达到很好的效果。\n",
    "\n",
    "### 模型结构\n",
    "\n",
    "ViT模型的主体结构是基于Transformer模型的Encoder部分（部分结构顺序有调整，如：Normalization的位置与标准Transformer不同），其结构图[1]如下：\n",
    "\n",
    "![vit-architecture](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/tutorials/application/source_zh_cn/cv/images/vit_architecture.png)\n",
    "\n",
    "### 模型特点\n",
    "\n",
    "ViT模型主要应用于图像分类领域。因此，其模型结构相较于传统的Transformer有以下几个特点：\n",
    "\n",
    "1. 数据集的原图像被划分为多个patch（图像块）后，将二维patch（不考虑channel）转换为一维向量，再加上类别向量与位置向量作为模型输入。\n",
    "2. 模型主体的Block结构是基于Transformer的Encoder结构，但是调整了Normalization的位置，其中，最主要的结构依然是Multi-head Attention结构。\n",
    "3. 模型在Blocks堆叠后接全连接层，接受类别向量的输出作为输入并用于分类。通常情况下，我们将最后的全连接层称为Head，Transformer Encoder部分为backbone。\n",
    "\n",
    "下面将通过代码实例来详细解释基于ViT实现ImageNet分类任务。\n",
    "\n",
    "> 注意，本教程在CPU上运行时间过长，不建议使用CPU运行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 环境准备与数据读取\n",
    "\n",
    "开始实验之前，请确保本地已经安装了Python环境并安装了MindSpore。\n",
    "\n",
    "首先我们需要下载本案例的数据集，可通过<http://image-net.org>下载完整的ImageNet数据集，本案例应用的数据集是从ImageNet中筛选出来的子集。\n",
    "\n",
    "运行第一段代码时会自动下载并解压，请确保你的数据集路径如以下结构。\n",
    "\n",
    "```text\n",
    ".dataset/\n",
    "    ├── ILSVRC2012_devkit_t12.tar.gz\n",
    "    ├── train/\n",
    "    ├── infer/\n",
    "    └── val/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "# 实验环境已经预装了mindspore==2.3.0，如需更换mindspore版本，可更改下面 MINDSPORE_VERSION 变量\n",
    "!pip uninstall mindspore -y\n",
    "!export MINDSPORE_VERSION=2.3.0\n",
    "!pip install https://ms-release.obs.cn-north-4.myhuaweicloud.com/${MINDSPORE_VERSION}/MindSpore/unified/aarch64/mindspore-${MINDSPORE_VERSION}-cp39-cp39-linux_aarch64.whl --trusted-host ms-release.obs.cn-north-4.myhuaweicloud.com -i https://pypi.mirrors.ustc.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: mindspore\n",
      "Version: 2.3.0\n",
      "Summary: MindSpore is a new open source deep learning training/inference framework that could be used for mobile, edge and cloud scenarios.\n",
      "Home-page: https://www.mindspore.cn\n",
      "Author: The MindSpore Authors\n",
      "Author-email: contact@mindspore.cn\n",
      "License: Apache 2.0\n",
      "Location: /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages\n",
      "Requires: asttokens, astunparse, numpy, packaging, pillow, protobuf, psutil, scipy\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# 查看当前 mindspore 版本\n",
    "!pip show mindspore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image', 'label']\n",
      "读取到数据\n",
      "数据集加载成功！\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mindspore as ms\n",
    "from mindspore.dataset import ImageFolderDataset\n",
    "import mindspore.dataset.vision as transforms\n",
    "\n",
    "# 修改数据集路径为相对路径\n",
    "data_path = 'dataset'\n",
    "\n",
    "# 定义均值和标准差\n",
    "mean = [0.485 * 255, 0.456 * 255, 0.406 * 255]\n",
    "std = [0.229 * 255, 0.224 * 255, 0.225 * 255]\n",
    "\n",
    "# 创建训练数据集\n",
    "dataset_train = ImageFolderDataset(os.path.join(data_path, \"train\"), shuffle=True)\n",
    "\n",
    "# 定义数据增强和预处理操作\n",
    "trans_train = [\n",
    "    transforms.RandomCropDecodeResize(size=224, scale=(0.08, 1.0), ratio=(0.75, 1.333)),\n",
    "    transforms.RandomHorizontalFlip(prob=0.5),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "    transforms.HWC2CHW()\n",
    "]\n",
    "\n",
    "# 应用数据增强和预处理操作\n",
    "dataset_train = dataset_train.map(operations=trans_train, input_columns=[\"image\"])\n",
    "\n",
    "# 设置批处理大小\n",
    "dataset_train = dataset_train.batch(batch_size=16, drop_remainder=True)\n",
    "\n",
    "# 获取数据集的列名\n",
    "print(dataset_train.get_col_names())\n",
    "\n",
    "# 验证数据集是否成功加载\n",
    "try:\n",
    "    for data in dataset_train.create_dict_iterator(num_epochs=1):\n",
    "        print(\"读取到数据\")\n",
    "        break\n",
    "    print(\"数据集加载成功！\")\n",
    "except Exception as e:\n",
    "    print(f\"加载数据集时出错：{str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(32:281473594719104,MainProcess):2024-08-29-12:08:57.943.874 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(32:281473594719104,MainProcess):2024-08-29-12:08:57.945.131 [mindspore/dataset/core/validator_helpers.py:744] 'RandomCrop' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'RandomCrop' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(32:281473594719104,MainProcess):2024-08-29-12:08:57.945.779 [mindspore/dataset/core/validator_helpers.py:744] 'RandomHorizontalFlip' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'RandomHorizontalFlip' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(32:281473594719104,MainProcess):2024-08-29-12:08:57.946.501 [mindspore/dataset/core/validator_helpers.py:744] 'Normalize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Normalize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(32:281473594719104,MainProcess):2024-08-29-12:08:57.947.090 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(32:281473594719104,MainProcess):2024-08-29-12:08:57.948.239 [mindspore/dataset/core/validator_helpers.py:744] 'OneHot' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'OneHot' from mindspore.dataset.transforms instead.\n",
      "[WARNING] ME(32:281473594719104,MainProcess):2024-08-29-12:08:57.953.381 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(32:281473594719104,MainProcess):2024-08-29-12:08:57.954.050 [mindspore/dataset/core/validator_helpers.py:744] 'RandomCrop' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'RandomCrop' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(32:281473594719104,MainProcess):2024-08-29-12:08:57.954.644 [mindspore/dataset/core/validator_helpers.py:744] 'RandomHorizontalFlip' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'RandomHorizontalFlip' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(32:281473594719104,MainProcess):2024-08-29-12:08:57.955.212 [mindspore/dataset/core/validator_helpers.py:744] 'Normalize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Normalize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(32:281473594719104,MainProcess):2024-08-29-12:08:57.955.899 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(32:281473594719104,MainProcess):2024-08-29-12:08:57.956.897 [mindspore/dataset/core/validator_helpers.py:744] 'OneHot' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'OneHot' from mindspore.dataset.transforms instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集列名: ['image', 'label']\n",
      "训练集大小: 23\n",
      "验证集列名: ['image', 'label']\n",
      "验证集大小: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] MD(32,fffd724cf120,python):2024-08-29-12:08:57.968.873 [mindspore/ccsrc/minddata/dataset/util/task_manager.cc:230] InterruptMaster] MindSpore dataset is terminated with err msg: Exception thrown from dataset pipeline. Refer to 'Dataset Pipeline Error Message'. map operation: [Resize] failed. The corresponding data file is: ./dataset/train/neg/L536.jpg. Resize: the image tensor should have at least two dimensions. You may need to perform Decode first.\n",
      "Line of code : 174\n",
      "File         : mindspore/ccsrc/minddata/dataset/kernels/image/image_utils.cc\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Exception thrown from dataset pipeline. Refer to 'Dataset Pipeline Error Message'. \n\n------------------------------------------------------------------\n- Dataset Pipeline Error Message: \n------------------------------------------------------------------\n[ERROR] map operation: [Resize] failed. The corresponding data file is: ./dataset/train/neg/L536.jpg. Resize: the image tensor should have at least two dimensions. You may need to perform Decode first.\n\n------------------------------------------------------------------\n- C++ Call Stack: (For framework developers) \n------------------------------------------------------------------\nmindspore/ccsrc/minddata/dataset/kernels/image/image_utils.cc(174).\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m验证集大小:\u001b[39m\u001b[38;5;124m\"\u001b[39m, val_dataset\u001b[38;5;241m.\u001b[39mget_dataset_size())\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# 获取一个批次的数据样本\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_dataset\u001b[38;5;241m.\u001b[39mcreate_dict_iterator():\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m图像形状:\u001b[39m\u001b[38;5;124m\"\u001b[39m, data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m标签形状:\u001b[39m\u001b[38;5;124m\"\u001b[39m, data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/site-packages/mindspore/dataset/engine/iterators.py:152\u001b[0m, in \u001b[0;36mIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterator does not have a running C++ pipeline.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# Note offload is applied inside _get_next() if applicable since get_next converts to output format\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__index \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/site-packages/mindspore/dataset/engine/iterators.py:277\u001b[0m, in \u001b[0;36mDictIterator._get_next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    275\u001b[0m     logger\u001b[38;5;241m.\u001b[39mcritical(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMemory error occurred, process will exit.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    276\u001b[0m     os\u001b[38;5;241m.\u001b[39mkill(os\u001b[38;5;241m.\u001b[39mgetpid(), signal\u001b[38;5;241m.\u001b[39mSIGKILL)\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/site-packages/mindspore/dataset/engine/iterators.py:260\u001b[0m, in \u001b[0;36mDictIterator._get_next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffload_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {k: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_md_to_output(t) \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGetNextAsMap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    261\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_md_to_tensor(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\u001b[38;5;241m.\u001b[39mGetNextAsList()]\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Exception thrown from dataset pipeline. Refer to 'Dataset Pipeline Error Message'. \n\n------------------------------------------------------------------\n- Dataset Pipeline Error Message: \n------------------------------------------------------------------\n[ERROR] map operation: [Resize] failed. The corresponding data file is: ./dataset/train/neg/L536.jpg. Resize: the image tensor should have at least two dimensions. You may need to perform Decode first.\n\n------------------------------------------------------------------\n- C++ Call Stack: (For framework developers) \n------------------------------------------------------------------\nmindspore/ccsrc/minddata/dataset/kernels/image/image_utils.cc(174).\n\n\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.vision.c_transforms as C\n",
    "import mindspore.dataset.transforms.c_transforms as C2\n",
    "\n",
    "# 设置数据集路径\n",
    "dataset_dir = './dataset'\n",
    "\n",
    "# 定义数据增强和预处理操作\n",
    "def create_dataset(data_path, batch_size=32, repeat_size=1, num_parallel_workers=8):\n",
    "    # 图像标准化参数\n",
    "    mean = [0.485 * 255, 0.456 * 255, 0.406 * 255]\n",
    "    std = [0.229 * 255, 0.224 * 255, 0.225 * 255]\n",
    "\n",
    "    # 数据增强操作\n",
    "    transform_img = [\n",
    "        C.Resize((256, 256)),\n",
    "        C.RandomCrop((224, 224)),\n",
    "        C.RandomHorizontalFlip(prob=0.5),\n",
    "        C.Normalize(mean=mean, std=std),\n",
    "        C.HWC2CHW()\n",
    "    ]\n",
    "\n",
    "    # 创建数据集\n",
    "    dataset = ds.ImageFolderDataset(data_path, num_parallel_workers=num_parallel_workers, shuffle=True)\n",
    "    \n",
    "    # 应用数据增强\n",
    "    dataset = dataset.map(operations=transform_img, input_columns=\"image\")\n",
    "    \n",
    "    # 对标签进行独热编码\n",
    "    one_hot_op = C2.OneHot(num_classes=2)  # 假设有两个类别：pos和neg\n",
    "    dataset = dataset.map(operations=one_hot_op, input_columns=[\"label\"])\n",
    "\n",
    "    # 批处理\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.repeat(repeat_size)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# 创建训练集\n",
    "train_dataset = create_dataset(os.path.join(dataset_dir, 'train'))\n",
    "print(\"训练集列名:\", train_dataset.get_col_names())\n",
    "print(\"训练集大小:\", train_dataset.get_dataset_size())\n",
    "\n",
    "# 创建验证集\n",
    "val_dataset = create_dataset(os.path.join(dataset_dir, 'val'))\n",
    "print(\"验证集列名:\", val_dataset.get_col_names())\n",
    "print(\"验证集大小:\", val_dataset.get_dataset_size())\n",
    "\n",
    "# 获取一个批次的数据样本\n",
    "for data in train_dataset.create_dict_iterator():\n",
    "    print(\"图像形状:\", data['image'].shape)\n",
    "    print(\"标签形状:\", data['label'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集信息：\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BatchDataset' object has no attribute 'get_column_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# 验证数据集\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m训练集信息：\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m列名: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_train\u001b[38;5;241m.\u001b[39mget_column_names()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m数据集大小: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_train\u001b[38;5;241m.\u001b[39mget_dataset_size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m类别: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_train\u001b[38;5;241m.\u001b[39mget_class_indexing()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BatchDataset' object has no attribute 'get_column_names'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mindspore as ms\n",
    "from mindspore.dataset import ImageFolderDataset\n",
    "import mindspore.dataset.vision as transforms\n",
    "\n",
    "# 设置数据集路径\n",
    "data_path = './dataset'\n",
    "\n",
    "# 定义均值和标准差\n",
    "mean = [0.485 * 255, 0.456 * 255, 0.406 * 255]\n",
    "std = [0.229 * 255, 0.224 * 255, 0.225 * 255]\n",
    "\n",
    "# 创建训练数据集\n",
    "dataset_train = ImageFolderDataset(os.path.join(data_path, \"train\"), shuffle=True)\n",
    "\n",
    "# 定义数据增强和预处理操作\n",
    "trans_train = [\n",
    "    transforms.RandomCropDecodeResize(size=224, scale=(0.08, 1.0), ratio=(0.75, 1.333)),\n",
    "    transforms.RandomHorizontalFlip(prob=0.5),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "    transforms.HWC2CHW()\n",
    "]\n",
    "\n",
    "# 应用数据增强和预处理操作\n",
    "dataset_train = dataset_train.map(operations=trans_train, input_columns=[\"image\"])\n",
    "\n",
    "# 设置批处理大小\n",
    "batch_size = 16\n",
    "dataset_train = dataset_train.batch(batch_size=batch_size, drop_remainder=True)\n",
    "\n",
    "# 验证数据集\n",
    "print(\"训练集信息：\")\n",
    "print(f\"列名: {dataset_train.get_column_names()}\")\n",
    "print(f\"数据集大小: {dataset_train.get_dataset_size()}\")\n",
    "print(f\"类别: {dataset_train.get_class_indexing()}\")\n",
    "\n",
    "# 计算原始图片数量\n",
    "train_dir = os.path.join(data_path, \"train\")\n",
    "total_images = sum([len(files) for r, d, files in os.walk(train_dir) if any(file.lower().endswith(('.png', '.jpg', '.jpeg')) for file in files)])\n",
    "print(f\"训练集中的总图片数量: {total_images}\")\n",
    "\n",
    "# 验证每个批次的数据\n",
    "print(\"\\n验证批次数据：\")\n",
    "for i, data in enumerate(dataset_train.create_dict_iterator(num_epochs=1)):\n",
    "    print(f\"批次 {i+1}:\")\n",
    "    print(f\"  图像形状: {data['image'].shape}\")\n",
    "    print(f\"  标签: {data['label']}\")\n",
    "    if i == 2:  # 只打印前3个批次的信息\n",
    "        break\n",
    "\n",
    "# 创建验证数据集\n",
    "dataset_val = ImageFolderDataset(os.path.join(data_path, \"val\"), shuffle=False)\n",
    "dataset_val = dataset_val.map(operations=trans_train, input_columns=[\"image\"])\n",
    "dataset_val = dataset_val.batch(batch_size=batch_size)\n",
    "\n",
    "print(\"\\n验证集信息：\")\n",
    "print(f\"列名: {dataset_val.get_col_names()}\")\n",
    "print(f\"数据集大小: {dataset_val.get_dataset_size()}\")\n",
    "print(f\"类别: {dataset_val.get_class_indexing()}\")\n",
    "\n",
    "# 计算验证集原始图片数量\n",
    "val_dir = os.path.join(data_path, \"val\")\n",
    "val_total_images = sum([len(files) for r, d, files in os.walk(val_dir) if any(file.lower().endswith(('.png', '.jpg', '.jpeg')) for file in files)])\n",
    "print(f\"验证集中的总图片数量: {val_total_images}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(32:281473594719104,MainProcess):2024-08-29-12:09:27.351.465 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(32:281473594719104,MainProcess):2024-08-29-12:09:27.352.382 [mindspore/dataset/core/validator_helpers.py:744] 'RandomCrop' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'RandomCrop' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(32:281473594719104,MainProcess):2024-08-29-12:09:27.353.132 [mindspore/dataset/core/validator_helpers.py:744] 'RandomHorizontalFlip' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'RandomHorizontalFlip' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(32:281473594719104,MainProcess):2024-08-29-12:09:27.353.774 [mindspore/dataset/core/validator_helpers.py:744] 'Normalize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Normalize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(32:281473594719104,MainProcess):2024-08-29-12:09:27.354.460 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(32:281473594719104,MainProcess):2024-08-29-12:09:27.356.328 [mindspore/dataset/core/validator_helpers.py:744] 'OneHot' from mindspore.dataset.transforms.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'OneHot' from mindspore.dataset.transforms instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集信息：\n",
      "类别: {'.ipynb_checkpoints': 0, 'neg': 1, 'pos': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] MD(32,fffd3c78f120,python):2024-08-29-12:09:27.368.315 [mindspore/ccsrc/minddata/dataset/util/task_manager.cc:230] InterruptMaster] MindSpore dataset is terminated with err msg: Exception thrown from dataset pipeline. Refer to 'Dataset Pipeline Error Message'. map operation: [Resize] failed. The corresponding data file is: ./dataset/train/pos/M-190.png. Resize: the image tensor should have at least two dimensions. You may need to perform Decode first.\n",
      "Line of code : 174\n",
      "File         : mindspore/ccsrc/minddata/dataset/kernels/image/image_utils.cc\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Exception thrown from dataset pipeline. Refer to 'Dataset Pipeline Error Message'. \n\n------------------------------------------------------------------\n- Dataset Pipeline Error Message: \n------------------------------------------------------------------\n[ERROR] map operation: [Resize] failed. The corresponding data file is: ./dataset/train/pos/M-190.png. Resize: the image tensor should have at least two dimensions. You may need to perform Decode first.\n\n------------------------------------------------------------------\n- C++ Call Stack: (For framework developers) \n------------------------------------------------------------------\nmindspore/ccsrc/minddata/dataset/kernels/image/image_utils.cc(174).\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 计算总样本数\u001b[39;00m\n\u001b[1;32m     47\u001b[0m total_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m train_dataset\u001b[38;5;241m.\u001b[39mcreate_dict_iterator():\n\u001b[1;32m     49\u001b[0m     total_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     50\u001b[0m total_samples \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mget_batch_size()\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/site-packages/mindspore/dataset/engine/iterators.py:152\u001b[0m, in \u001b[0;36mIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterator does not have a running C++ pipeline.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# Note offload is applied inside _get_next() if applicable since get_next converts to output format\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__index \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/site-packages/mindspore/dataset/engine/iterators.py:277\u001b[0m, in \u001b[0;36mDictIterator._get_next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    275\u001b[0m     logger\u001b[38;5;241m.\u001b[39mcritical(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMemory error occurred, process will exit.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    276\u001b[0m     os\u001b[38;5;241m.\u001b[39mkill(os\u001b[38;5;241m.\u001b[39mgetpid(), signal\u001b[38;5;241m.\u001b[39mSIGKILL)\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/site-packages/mindspore/dataset/engine/iterators.py:260\u001b[0m, in \u001b[0;36mDictIterator._get_next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffload_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {k: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_md_to_output(t) \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGetNextAsMap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    261\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_md_to_tensor(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\u001b[38;5;241m.\u001b[39mGetNextAsList()]\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Exception thrown from dataset pipeline. Refer to 'Dataset Pipeline Error Message'. \n\n------------------------------------------------------------------\n- Dataset Pipeline Error Message: \n------------------------------------------------------------------\n[ERROR] map operation: [Resize] failed. The corresponding data file is: ./dataset/train/pos/M-190.png. Resize: the image tensor should have at least two dimensions. You may need to perform Decode first.\n\n------------------------------------------------------------------\n- C++ Call Stack: (For framework developers) \n------------------------------------------------------------------\nmindspore/ccsrc/minddata/dataset/kernels/image/image_utils.cc(174).\n\n\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.vision.c_transforms as C\n",
    "import mindspore.dataset.transforms.c_transforms as C2\n",
    "\n",
    "# 设置数据集路径\n",
    "dataset_dir = './dataset'\n",
    "\n",
    "def create_dataset(data_path, batch_size=32, repeat_size=1, num_parallel_workers=8):\n",
    "    # 图像标准化参数\n",
    "    mean = [0.485 * 255, 0.456 * 255, 0.406 * 255]\n",
    "    std = [0.229 * 255, 0.224 * 255, 0.225 * 255]\n",
    "\n",
    "    # 数据增强操作\n",
    "    transform_img = [\n",
    "        C.Resize((256, 256)),\n",
    "        C.RandomCrop((224, 224)),\n",
    "        C.RandomHorizontalFlip(prob=0.5),\n",
    "        C.Normalize(mean=mean, std=std),\n",
    "        C.HWC2CHW()\n",
    "    ]\n",
    "\n",
    "    # 创建数据集\n",
    "    dataset = ds.ImageFolderDataset(data_path, num_parallel_workers=num_parallel_workers, shuffle=True)\n",
    "    \n",
    "    # 应用数据增强\n",
    "    dataset = dataset.map(operations=transform_img, input_columns=\"image\")\n",
    "    \n",
    "    # 对标签进行独热编码\n",
    "    one_hot_op = C2.OneHot(num_classes=2)  # 假设有两个类别：pos和neg\n",
    "    dataset = dataset.map(operations=one_hot_op, input_columns=[\"label\"])\n",
    "\n",
    "    # 批处理\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.repeat(repeat_size)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# 创建训练集\n",
    "train_dataset = create_dataset(os.path.join(dataset_dir, 'train'))\n",
    "\n",
    "# 验证数据集\n",
    "print(\"训练集信息：\")\n",
    "print(f\"类别: {train_dataset.get_class_indexing()}\")\n",
    "\n",
    "# 计算总样本数\n",
    "total_samples = 0\n",
    "for _ in train_dataset.create_dict_iterator():\n",
    "    total_samples += 1\n",
    "total_samples *= train_dataset.get_batch_size()\n",
    "\n",
    "print(f\"总样本数: {total_samples}\")\n",
    "print(f\"批次大小: {train_dataset.get_batch_size()}\")\n",
    "print(f\"批次数量: {train_dataset.get_dataset_size()}\")\n",
    "\n",
    "# 验证原始图片数量\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "total_images = sum([len(files) for r, d, files in os.walk(train_dir) if files])\n",
    "print(f\"原始图片总数: {total_images}\")\n",
    "\n",
    "# 获取一个批次的数据样本\n",
    "for data in train_dataset.create_dict_iterator():\n",
    "    print(\"图像形状:\", data['image'].shape)\n",
    "    print(\"标签形状:\", data['label'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(7803:281473386118016,MainProcess):2024-08-29-13:30:52.251.136 [mindspore/dataset/core/validator_helpers.py:744] 'Decode' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Decode' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7803:281473386118016,MainProcess):2024-08-29-13:30:52.252.160 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7803:281473386118016,MainProcess):2024-08-29-13:30:52.252.941 [mindspore/dataset/core/validator_helpers.py:744] 'RandomCrop' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'RandomCrop' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7803:281473386118016,MainProcess):2024-08-29-13:30:52.253.735 [mindspore/dataset/core/validator_helpers.py:744] 'RandomHorizontalFlip' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'RandomHorizontalFlip' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7803:281473386118016,MainProcess):2024-08-29-13:30:52.254.448 [mindspore/dataset/core/validator_helpers.py:744] 'Normalize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Normalize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7803:281473386118016,MainProcess):2024-08-29-13:30:52.255.289 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "训练集信息：\n",
      "数据集大小: 23\n",
      "训练集中的总图片数量: 742\n",
      "\n",
      "验证训练集批次数据：\n",
      "批次 1:\n",
      "  图像形状: (32, 3, 224, 224)\n",
      "  标签: [1 2 1 1 1 1 1 2 1 1 1 1 2 1 1 2 1 1 2 2 1 2 1 1 1 1 2 2 1 2 1 2]\n",
      "批次 2:\n",
      "  图像形状: (32, 3, 224, 224)\n",
      "  标签: [1 1 2 2 1 1 1 2 1 2 1 1 1 2 1 1 1 2 2 1 1 2 1 1 2 1 2 1 1 1 2 2]\n",
      "批次 3:\n",
      "  图像形状: (32, 3, 224, 224)\n",
      "  标签: [1 2 1 2 2 2 1 1 2 1 2 2 1 1 2 1 1 2 2 1 1 1 1 1 1 1 1 1 2 1 2 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(7803:281473386118016,MainProcess):2024-08-29-13:30:53.196.252 [mindspore/dataset/core/validator_helpers.py:744] 'Decode' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Decode' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7803:281473386118016,MainProcess):2024-08-29-13:30:53.197.062 [mindspore/dataset/core/validator_helpers.py:744] 'Resize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Resize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7803:281473386118016,MainProcess):2024-08-29-13:30:53.197.807 [mindspore/dataset/core/validator_helpers.py:744] 'Normalize' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'Normalize' from mindspore.dataset.vision instead.\n",
      "[WARNING] ME(7803:281473386118016,MainProcess):2024-08-29-13:30:53.198.421 [mindspore/dataset/core/validator_helpers.py:744] 'HWC2CHW' from mindspore.dataset.vision.c_transforms is deprecated from version 1.8 and will be removed in a future version. Use 'HWC2CHW' from mindspore.dataset.vision instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "验证集信息：\n",
      "数据集大小: 13\n",
      "验证集中的总图片数量: 434\n",
      "\n",
      "验证验证集批次数据：\n",
      "批次 1:\n",
      "  图像形状: (32, 3, 224, 224)\n",
      "  标签: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "批次 2:\n",
      "  图像形状: (32, 3, 224, 224)\n",
      "  标签: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "批次 3:\n",
      "  图像形状: (32, 3, 224, 224)\n",
      "  标签: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.vision.c_transforms as C\n",
    "import mindspore.dataset.transforms.c_transforms as C2\n",
    "\n",
    "# 设置数据集路径\n",
    "data_path = './dataset'\n",
    "\n",
    "# 定义数据增强和预处理操作\n",
    "def create_dataset(data_path, batch_size=32, is_training=True):\n",
    "    # 图像标准化参数\n",
    "    mean = [0.485 * 255, 0.456 * 255, 0.406 * 255]\n",
    "    std = [0.229 * 255, 0.224 * 255, 0.225 * 255]\n",
    "\n",
    "    # 数据增强操作\n",
    "    if is_training:\n",
    "        transform_img = [\n",
    "            C.Decode(),\n",
    "            C.Resize((256, 256)),\n",
    "            C.RandomCrop((224, 224)),\n",
    "            C.RandomHorizontalFlip(prob=0.5),\n",
    "            C.Normalize(mean=mean, std=std),\n",
    "            C.HWC2CHW()\n",
    "        ]\n",
    "    else:\n",
    "        transform_img = [\n",
    "            C.Decode(),\n",
    "            C.Resize((224, 224)),\n",
    "            C.Normalize(mean=mean, std=std),\n",
    "            C.HWC2CHW()\n",
    "        ]\n",
    "\n",
    "    # 创建数据集\n",
    "    dataset = ds.ImageFolderDataset(data_path, num_parallel_workers=8, shuffle=is_training)\n",
    "    \n",
    "    # 应用数据增强\n",
    "    dataset = dataset.map(operations=transform_img, input_columns=\"image\")\n",
    "    \n",
    "    # 批处理\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# 创建训练集\n",
    "train_dataset = create_dataset(os.path.join(data_path, 'train'), is_training=True)\n",
    "print(\"\\n训练集信息：\")\n",
    "print(f\"数据集大小: {train_dataset.get_dataset_size()}\")\n",
    "\n",
    "# 计算训练集原始图片数量\n",
    "train_dir = os.path.join(data_path, \"train\")\n",
    "train_total_images = sum([len(files) for r, d, files in os.walk(train_dir) if any(file.lower().endswith(('.png', '.jpg', '.jpeg')) for file in files)])\n",
    "print(f\"训练集中的总图片数量: {train_total_images}\")\n",
    "\n",
    "# 验证每个批次的数据\n",
    "print(\"\\n验证训练集批次数据：\")\n",
    "for i, data in enumerate(train_dataset.create_dict_iterator(num_epochs=1)):\n",
    "    print(f\"批次 {i+1}:\")\n",
    "    print(f\"  图像形状: {data['image'].shape}\")\n",
    "    print(f\"  标签: {data['label']}\")\n",
    "    if i == 2:  # 只打印前3个批次的信息\n",
    "        break\n",
    "\n",
    "# 创建验证数据集\n",
    "val_dataset = create_dataset(os.path.join(data_path, 'val'), is_training=False)\n",
    "print(\"\\n验证集信息：\")\n",
    "print(f\"数据集大小: {val_dataset.get_dataset_size()}\")\n",
    "\n",
    "# 计算验证集原始图片数量\n",
    "val_dir = os.path.join(data_path, \"val\")\n",
    "val_total_images = sum([len(files) for r, d, files in os.walk(val_dir) if any(file.lower().endswith(('.png', '.jpg', '.jpeg')) for file in files)])\n",
    "print(f\"验证集中的总图片数量: {val_total_images}\")\n",
    "\n",
    "# 验证验证集批次数据\n",
    "print(\"\\n验证验证集批次数据：\")\n",
    "for i, data in enumerate(val_dataset.create_dict_iterator(num_epochs=1)):\n",
    "    print(f\"批次 {i+1}:\")\n",
    "    print(f\"  图像形状: {data['image'].shape}\")\n",
    "    print(f\"  标签: {data['label']}\")\n",
    "    if i == 2:  # 只打印前3个批次的信息\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 模型解析\n",
    "\n",
    "下面将通过代码来细致剖析ViT模型的内部结构。\n",
    "\n",
    "### Transformer基本原理\n",
    "\n",
    "Transformer模型源于2017年的一篇文章[2]。在这篇文章中提出的基于Attention机制的编码器-解码器型结构在自然语言处理领域获得了巨大的成功。模型结构如下图所示：\n",
    "\n",
    "![transformer-architecture](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/tutorials/application/source_zh_cn/cv/images/transformer_architecture.png)\n",
    "\n",
    "其主要结构为多个Encoder和Decoder模块所组成，其中Encoder和Decoder的详细结构如下图[2]所示：\n",
    "\n",
    "![encoder-decoder](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/tutorials/application/source_zh_cn/cv/images/encoder_decoder.png)\n",
    "\n",
    "Encoder与Decoder由许多结构组成，如：多头注意力（Multi-Head Attention）层，Feed Forward层，Normaliztion层，甚至残差连接（Residual Connection，图中的“Add”）。不过，其中最重要的结构是多头注意力（Multi-Head Attention）结构，该结构基于自注意力（Self-Attention）机制，是多个Self-Attention的并行组成。\n",
    "\n",
    "所以，理解了Self-Attention就抓住了Transformer的核心。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Attention模块\n",
    "\n",
    "以下是Self-Attention的解释，其核心内容是为输入向量的每个单词学习一个权重。通过给定一个任务相关的查询向量Query向量，计算Query和各个Key的相似性或者相关性得到注意力分布，即得到每个Key对应Value的权重系数，然后对Value进行加权求和得到最终的Attention数值。\n",
    "\n",
    "在Self-Attention中：\n",
    "\n",
    "1. 最初的输入向量首先会经过Embedding层映射成Q（Query），K（Key），V（Value）三个向量，由于是并行操作，所以代码中是映射成为dim x 3的向量然后进行分割，换言之，如果你的输入向量为一个向量序列（$x_1$，$x_2$，$x_3$），其中的$x_1$，$x_2$，$x_3$都是一维向量，那么每一个一维向量都会经过Embedding层映射出Q，K，V三个向量，只是Embedding矩阵不同，矩阵参数也是通过学习得到的。**这里大家可以认为，Q，K，V三个矩阵是发现向量之间关联信息的一种手段，需要经过学习得到，至于为什么是Q，K，V三个，主要是因为需要两个向量点乘以获得权重，又需要另一个向量来承载权重向加的结果，所以，最少需要3个矩阵。**\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "q_i = W_q \\cdot x_i & \\\\\n",
    "k_i = W_k \\cdot x_i,\\hspace{1em} &i = 1,2,3 \\ldots \\\\\n",
    "v_i = W_v \\cdot x_i &\n",
    "\\end{cases}\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "![self-attention1](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/tutorials/application/source_zh_cn/cv/images/self_attention_1.png)\n",
    "\n",
    "2. 自注意力机制的自注意主要体现在它的Q，K，V都来源于其自身，也就是该过程是在提取输入的不同顺序的向量的联系与特征，最终通过不同顺序向量之间的联系紧密性（Q与K乘积经过Softmax的结果）来表现出来。**Q，K，V得到后就需要获取向量间权重，需要对Q和K进行点乘并除以维度的平方根，对所有向量的结果进行Softmax处理，通过公式(2)的操作，我们获得了向量之间的关系权重。**\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "a_{1,1} = q_1 \\cdot k_1 / \\sqrt d \\\\\n",
    "a_{1,2} = q_1 \\cdot k_2 / \\sqrt d \\\\\n",
    "a_{1,3} = q_1 \\cdot k_3 / \\sqrt d\n",
    "\\end{cases}\n",
    "\\tag{2}\n",
    "$$\n",
    "\n",
    "![self-attention3](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/tutorials/application/source_zh_cn/cv/images/self_attention_3.png)\n",
    "\n",
    "$$ Softmax: \\hat a_{1,i} = exp(a_{1,i}) / \\sum_j exp(a_{1,j}),\\hspace{1em} j = 1,2,3 \\ldots \\tag{3}$$\n",
    "\n",
    "![self-attention2](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/tutorials/application/source_zh_cn/cv/images/self_attention_2.png)\n",
    "\n",
    "3. 其最终输出则是通过V这个映射后的向量与Q，K经过Softmax结果进行weight sum获得，这个过程可以理解为在全局上进行自注意表示。**每一组Q，K，V最后都有一个V输出，这是Self-Attention得到的最终结果，是当前向量在结合了它与其他向量关联权重后得到的结果。**\n",
    "\n",
    "$$\n",
    "b_1 = \\sum_i \\hat a_{1,i}v_i,\\hspace{1em} i = 1,2,3...\n",
    "\\tag{4}\n",
    "$$\n",
    "\n",
    "通过下图可以整体把握Self-Attention的全部过程。\n",
    "\n",
    "![self-attention](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/tutorials/application/source_zh_cn/cv/images/self_attention_process.png)\n",
    "\n",
    "多头注意力机制就是将原本self-Attention处理的向量分割为多个Head进行处理，这一点也可以从代码中体现，这也是attention结构可以进行并行加速的一个方面。\n",
    "\n",
    "总结来说，多头注意力机制在保持参数总量不变的情况下，将同样的query, key和value映射到原来的高维空间（Q,K,V）的不同子空间(Q_0,K_0,V_0)中进行自注意力的计算，最后再合并不同子空间中的注意力信息。\n",
    "\n",
    "所以，对于同一个输入向量，多个注意力机制可以同时对其进行处理，即利用并行计算加速处理过程，又在处理的时候更充分的分析和利用了向量特征。下图展示了多头注意力机制，其并行能力的主要体现在下图中的$a_1$和$a_2$是同一个向量进行分割获得的。\n",
    "\n",
    "![multi-head-attention](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/tutorials/application/source_zh_cn/cv/images/multi_head_attention.png)\n",
    "\n",
    "以下是Multi-Head Attention代码，结合上文的解释，代码清晰的展现了这一过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mindspore import nn, ops\n",
    "\n",
    "\n",
    "class Attention(nn.Cell):\n",
    "    def __init__(self,\n",
    "                 dim: int,\n",
    "                 num_heads: int = 8,\n",
    "                 keep_prob: float = 1.0,\n",
    "                 attention_keep_prob: float = 1.0):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = ms.Tensor(head_dim ** -0.5)\n",
    "\n",
    "        self.qkv = nn.Dense(dim, dim * 3)\n",
    "        self.attn_drop = nn.Dropout(p=1.0-attention_keep_prob)\n",
    "        self.out = nn.Dense(dim, dim)\n",
    "        self.out_drop = nn.Dropout(p=1.0-keep_prob)\n",
    "        self.attn_matmul_v = ops.BatchMatMul()\n",
    "        self.q_matmul_k = ops.BatchMatMul(transpose_b=True)\n",
    "        self.softmax = nn.Softmax(axis=-1)\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"Attention construct.\"\"\"\n",
    "        b, n, c = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "        qkv = ops.reshape(qkv, (b, n, 3, self.num_heads, c // self.num_heads))\n",
    "        qkv = ops.transpose(qkv, (2, 0, 3, 1, 4))\n",
    "        q, k, v = ops.unstack(qkv, axis=0)\n",
    "        attn = self.q_matmul_k(q, k)\n",
    "        attn = ops.mul(attn, self.scale)\n",
    "        attn = self.softmax(attn)\n",
    "        attn = self.attn_drop(attn)\n",
    "        out = self.attn_matmul_v(attn, v)\n",
    "        out = ops.transpose(out, (0, 2, 1, 3))\n",
    "        out = ops.reshape(out, (b, n, c))\n",
    "        out = self.out(out)\n",
    "        out = self.out_drop(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Transformer Encoder\n",
    "\n",
    "在了解了Self-Attention结构之后，通过与Feed Forward，Residual Connection等结构的拼接就可以形成Transformer的基础结构，下面代码实现了Feed Forward，Residual Connection结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Dict\n",
    "\n",
    "\n",
    "class FeedForward(nn.Cell):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 hidden_features: Optional[int] = None,\n",
    "                 out_features: Optional[int] = None,\n",
    "                 activation: nn.Cell = nn.GELU,\n",
    "                 keep_prob: float = 1.0):\n",
    "        super(FeedForward, self).__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.dense1 = nn.Dense(in_features, hidden_features)\n",
    "        self.activation = activation()\n",
    "        self.dense2 = nn.Dense(hidden_features, out_features)\n",
    "        self.dropout = nn.Dropout(p=1.0-keep_prob)\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"Feed Forward construct.\"\"\"\n",
    "        x = self.dense1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResidualCell(nn.Cell):\n",
    "    def __init__(self, cell):\n",
    "        super(ResidualCell, self).__init__()\n",
    "        self.cell = cell\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"ResidualCell construct.\"\"\"\n",
    "        return self.cell(x) + x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "接下来就利用Self-Attention来构建ViT模型中的TransformerEncoder部分，类似于构建了一个Transformer的编码器部分，如下图[1]所示：\n",
    "\n",
    "![vit-encoder](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/tutorials/application/source_zh_cn/cv/images/vit_encoder.png)\n",
    "\n",
    "1. ViT模型中的基础结构与标准Transformer有所不同，主要在于Normalization的位置是放在Self-Attention和Feed Forward之前，其他结构如Residual Connection，Feed Forward，Normalization都如Transformer中所设计。\n",
    "\n",
    "2. 从Transformer结构的图片可以发现，多个子encoder的堆叠就完成了模型编码器的构建，在ViT模型中，依然沿用这个思路，通过配置超参数num_layers，就可以确定堆叠层数。\n",
    "\n",
    "3. Residual Connection，Normalization的结构可以保证模型有很强的扩展性（保证信息经过深层处理不会出现退化的现象，这是Residual Connection的作用），Normalization和dropout的应用可以增强模型泛化能力。\n",
    "\n",
    "从以下源码中就可以清晰看到Transformer的结构。将TransformerEncoder结构和一个多层感知器（MLP）结合，就构成了ViT模型的backbone部分。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Cell):\n",
    "    def __init__(self,\n",
    "                 dim: int,\n",
    "                 num_layers: int,\n",
    "                 num_heads: int,\n",
    "                 mlp_dim: int,\n",
    "                 keep_prob: float = 1.,\n",
    "                 attention_keep_prob: float = 1.0,\n",
    "                 drop_path_keep_prob: float = 1.0,\n",
    "                 activation: nn.Cell = nn.GELU,\n",
    "                 norm: nn.Cell = nn.LayerNorm):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        layers = []\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            normalization1 = norm((dim,))\n",
    "            normalization2 = norm((dim,))\n",
    "            attention = Attention(dim=dim,\n",
    "                                  num_heads=num_heads,\n",
    "                                  keep_prob=keep_prob,\n",
    "                                  attention_keep_prob=attention_keep_prob)\n",
    "\n",
    "            feedforward = FeedForward(in_features=dim,\n",
    "                                      hidden_features=mlp_dim,\n",
    "                                      activation=activation,\n",
    "                                      keep_prob=keep_prob)\n",
    "\n",
    "            layers.append(\n",
    "                nn.SequentialCell([\n",
    "                    ResidualCell(nn.SequentialCell([normalization1, attention])),\n",
    "                    ResidualCell(nn.SequentialCell([normalization2, feedforward]))\n",
    "                ])\n",
    "            )\n",
    "        self.layers = nn.SequentialCell(layers)\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"Transformer construct.\"\"\"\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ViT模型的输入\n",
    "\n",
    "传统的Transformer结构主要用于处理自然语言领域的词向量（Word Embedding or Word Vector），词向量与传统图像数据的主要区别在于，词向量通常是一维向量进行堆叠，而图片则是二维矩阵的堆叠，多头注意力机制在处理一维词向量的堆叠时会提取词向量之间的联系也就是上下文语义，这使得Transformer在自然语言处理领域非常好用，而二维图片矩阵如何与一维词向量进行转化就成为了Transformer进军图像处理领域的一个小门槛。\n",
    "\n",
    "在ViT模型中：\n",
    "\n",
    "1. 通过将输入图像在每个channel上划分为16 x 16个patch，这一步是通过卷积操作来完成的，当然也可以人工进行划分，但卷积操作也可以达到目的同时还可以进行一次额外的数据处理；**例如一幅输入224 x 224的图像，首先经过卷积处理得到16 x 16个patch，那么每一个patch的大小就是14 x 14。**\n",
    "\n",
    "2. 再将每一个patch的矩阵拉伸成为一个一维向量，从而获得了近似词向量堆叠的效果。**上一步得到的14 x 14的patch就转换为长度为196的向量。**\n",
    "\n",
    "这是图像输入网络经过的第一步处理。具体Patch Embedding的代码如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Cell):\n",
    "    MIN_NUM_PATCHES = 4\n",
    "\n",
    "    def __init__(self,\n",
    "                 image_size: int = 224,\n",
    "                 patch_size: int = 16,\n",
    "                 embed_dim: int = 768,\n",
    "                 input_channels: int = 3):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.conv = nn.Conv2d(input_channels, embed_dim, kernel_size=patch_size, stride=patch_size, has_bias=True)\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"Path Embedding construct.\"\"\"\n",
    "        x = self.conv(x)\n",
    "        b, c, h, w = x.shape\n",
    "        x = ops.reshape(x, (b, c, h * w))\n",
    "        x = ops.transpose(x, (0, 2, 1))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "输入图像在划分为patch之后，会经过pos_embedding 和 class_embedding两个过程。\n",
    "\n",
    "1. class_embedding主要借鉴了BERT模型的用于文本分类时的思想，在每一个word vector之前增加一个类别值，通常是加在向量的第一位，**上一步得到的196维的向量加上class_embedding后变为197维。**\n",
    "\n",
    "2. 增加的class_embedding是一个可以学习的参数，经过网络的不断训练，最终以输出向量的第一个维度的输出来决定最后的输出类别；**由于输入是16 x 16个patch，所以输出进行分类时是取 16 x 16个class_embedding进行分类。**\n",
    "\n",
    "3. pos_embedding也是一组可以学习的参数，会被加入到经过处理的patch矩阵中。\n",
    "\n",
    "4. 由于pos_embedding也是可以学习的参数，所以它的加入类似于全链接网络和卷积的bias。**这一步就是创造一个长度维197的可训练向量加入到经过class_embedding的向量中。**\n",
    "\n",
    "实际上，pos_embedding总共有4种方案。但是经过作者的论证，只有加上pos_embedding和不加pos_embedding有明显影响，至于pos_embedding是一维还是二维对分类结果影响不大，所以，在我们的代码中，也是采用了一维的pos_embedding，由于class_embedding是加在pos_embedding之前，所以pos_embedding的维度会比patch拉伸后的维度加1。\n",
    "\n",
    "总的而言，ViT模型还是利用了Transformer模型在处理上下文语义时的优势，将图像转换为一种“变种词向量”然后进行处理，而这样转换的意义在于，多个patch之间本身具有空间联系，这类似于一种“空间语义”，从而获得了比较好的处理效果。\n",
    "\n",
    "### 整体构建ViT\n",
    "\n",
    "以下代码构建了一个完整的ViT模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mindspore.common.initializer import Normal\n",
    "from mindspore.common.initializer import initializer\n",
    "from mindspore import Parameter\n",
    "\n",
    "\n",
    "def init(init_type, shape, dtype, name, requires_grad):\n",
    "    \"\"\"Init.\"\"\"\n",
    "    initial = initializer(init_type, shape, dtype).init_data()\n",
    "    return Parameter(initial, name=name, requires_grad=requires_grad)\n",
    "\n",
    "\n",
    "class ViT(nn.Cell):\n",
    "    def __init__(self,\n",
    "                 image_size: int = 224,\n",
    "                 input_channels: int = 3,\n",
    "                 patch_size: int = 16,\n",
    "                 embed_dim: int = 768,\n",
    "                 num_layers: int = 12,\n",
    "                 num_heads: int = 12,\n",
    "                 mlp_dim: int = 3072,\n",
    "                 keep_prob: float = 1.0,\n",
    "                 attention_keep_prob: float = 1.0,\n",
    "                 drop_path_keep_prob: float = 1.0,\n",
    "                 activation: nn.Cell = nn.GELU,\n",
    "                 norm: Optional[nn.Cell] = nn.LayerNorm,\n",
    "                 pool: str = 'cls') -> None:\n",
    "        super(ViT, self).__init__()\n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(image_size=image_size,\n",
    "                                              patch_size=patch_size,\n",
    "                                              embed_dim=embed_dim,\n",
    "                                              input_channels=input_channels)\n",
    "        num_patches = self.patch_embedding.num_patches\n",
    "\n",
    "        self.cls_token = init(init_type=Normal(sigma=1.0),\n",
    "                              shape=(1, 1, embed_dim),\n",
    "                              dtype=ms.float32,\n",
    "                              name='cls',\n",
    "                              requires_grad=True)\n",
    "\n",
    "        self.pos_embedding = init(init_type=Normal(sigma=1.0),\n",
    "                                  shape=(1, num_patches + 1, embed_dim),\n",
    "                                  dtype=ms.float32,\n",
    "                                  name='pos_embedding',\n",
    "                                  requires_grad=True)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.pos_dropout = nn.Dropout(p=1.0-keep_prob)\n",
    "        self.norm = norm((embed_dim,))\n",
    "        self.transformer = TransformerEncoder(dim=embed_dim,\n",
    "                                              num_layers=num_layers,\n",
    "                                              num_heads=num_heads,\n",
    "                                              mlp_dim=mlp_dim,\n",
    "                                              keep_prob=keep_prob,\n",
    "                                              attention_keep_prob=attention_keep_prob,\n",
    "                                              drop_path_keep_prob=drop_path_keep_prob,\n",
    "                                              activation=activation,\n",
    "                                              norm=norm)\n",
    "        self.dropout = nn.Dropout(p=1.0-keep_prob)\n",
    "        self.dense = nn.Dense(embed_dim, num_classes)\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"ViT construct.\"\"\"\n",
    "        x = self.patch_embedding(x)\n",
    "        cls_tokens = ops.tile(self.cls_token.astype(x.dtype), (x.shape[0], 1, 1))\n",
    "        x = ops.concat((cls_tokens, x), axis=1)\n",
    "        x += self.pos_embedding\n",
    "\n",
    "        x = self.pos_dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.norm(x)\n",
    "        x = x[:, 0]\n",
    "        if self.training:\n",
    "            x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from download import download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "整体流程图如下所示：\n",
    "\n",
    "![data-process](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/tutorials/application/source_zh_cn/cv/images/data_process.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 模型训练与推理\n",
    "\n",
    "### 模型训练\n",
    "\n",
    "模型开始训练前，需要设定损失函数，优化器，回调函数等。\n",
    "\n",
    "完整训练ViT模型需要很长的时间，实际应用时建议根据项目需要调整epoch_size，当正常输出每个Epoch的step信息时，意味着训练正在进行，通过模型输出可以查看当前训练的loss值和时间等指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch time: 33565.661 ms, per step time: 729.688 ms\n",
      "Train epoch time: 10164.788 ms, per step time: 220.974 ms\n",
      "epoch: 3 step: 33, loss is 0.32509822\n",
      "Train epoch time: 10150.031 ms, per step time: 220.653 ms\n",
      "Train epoch time: 10124.424 ms, per step time: 220.096 ms\n",
      "Train epoch time: 10122.656 ms, per step time: 220.058 ms\n",
      "epoch: 6 step: 20, loss is 0.2743714\n",
      "Train epoch time: 10147.209 ms, per step time: 220.592 ms\n",
      "Train epoch time: 10130.362 ms, per step time: 220.225 ms\n",
      "Train epoch time: 10174.448 ms, per step time: 221.184 ms\n",
      "epoch: 9 step: 7, loss is 0.23837946\n",
      "Train epoch time: 10116.514 ms, per step time: 219.924 ms\n",
      "Train epoch time: 9813.018 ms, per step time: 213.326 ms\n",
      "CPU times: user 13min 7s, sys: 2min 24s, total: 15min 31s\n",
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from mindspore.nn import LossBase\n",
    "from mindspore.train import LossMonitor, TimeMonitor, CheckpointConfig, ModelCheckpoint\n",
    "from mindspore import train\n",
    "\n",
    "# define super parameter\n",
    "epoch_size = 10\n",
    "momentum = 0.9\n",
    "num_classes = 2\n",
    "resize = 224\n",
    "step_size = dataset_train.get_dataset_size()\n",
    "\n",
    "# construct model\n",
    "network = ViT()\n",
    "\n",
    "# load ckpt\n",
    "#vit_url = \"https://download.mindspore.cn/vision/classification/vit_b_16-10_46.ckpt\"\n",
    "#path = \"./ckpt/vit_b_16_224.ckpt\"\n",
    "\n",
    "#Vit_path = \"ViT/vit_b_16-10_46.ckpt\"\n",
    "#param_dict = ms.load_checkpoint(vit_path)\n",
    "#ms.load_param_into_net(network, param_dict)\n",
    "\n",
    "# define learning rate\n",
    "lr = nn.cosine_decay_lr(min_lr=float(0),\n",
    "                        max_lr=0.00005,\n",
    "                        total_step=epoch_size * step_size,\n",
    "                        step_per_epoch=step_size,\n",
    "                        decay_epoch=10)\n",
    "\n",
    "# define optimizer\n",
    "network_opt = nn.Adam(network.trainable_params(), lr, momentum)\n",
    "\n",
    "\n",
    "# define loss function\n",
    "class CrossEntropySmooth(LossBase):\n",
    "    \"\"\"CrossEntropy.\"\"\"\n",
    "\n",
    "    def __init__(self, sparse=True, reduction='mean', smooth_factor=0., num_classes=2):\n",
    "        super(CrossEntropySmooth, self).__init__()\n",
    "        self.onehot = ops.OneHot()\n",
    "        self.sparse = sparse\n",
    "        self.on_value = ms.Tensor(1.0 - smooth_factor, ms.float32)\n",
    "        self.off_value = ms.Tensor(1.0 * smooth_factor / (num_classes - 1), ms.float32)\n",
    "        self.ce = nn.SoftmaxCrossEntropyWithLogits(reduction=reduction)\n",
    "\n",
    "    def construct(self, logit, label):\n",
    "        if self.sparse:\n",
    "            label = self.onehot(label, ops.shape(logit)[1], self.on_value, self.off_value)\n",
    "        loss = self.ce(logit, label)\n",
    "        return loss\n",
    "\n",
    "\n",
    "network_loss = CrossEntropySmooth(sparse=True,\n",
    "                                  reduction=\"mean\",\n",
    "                                  smooth_factor=0.1,\n",
    "                                  num_classes=num_classes)\n",
    "\n",
    "# set checkpoint\n",
    "ckpt_config = CheckpointConfig(save_checkpoint_steps=step_size, keep_checkpoint_max=100)\n",
    "ckpt_callback = ModelCheckpoint(prefix='vit_b_16', directory='./ViT', config=ckpt_config)\n",
    "\n",
    "# initialize model\n",
    "# \"Ascend + mixed precision\" can improve performance\n",
    "ascend_target = (ms.get_context(\"device_target\") == \"Ascend\")\n",
    "if ascend_target:\n",
    "    model = train.Model(network, loss_fn=network_loss, optimizer=network_opt, metrics={\"acc\"}, amp_level=\"O2\")\n",
    "else:\n",
    "    model = train.Model(network, loss_fn=network_loss, optimizer=network_opt, metrics={\"acc\"}, amp_level=\"O0\")\n",
    "\n",
    "# train model\n",
    "model.train(epoch_size,\n",
    "            dataset_train,\n",
    "            callbacks=[ckpt_callback, LossMonitor(125), TimeMonitor(125)],\n",
    "            dataset_sink_mode=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 在评估模型之前添加一个简单的转换来检查和调整形状\n",
    "def check_shapes(images, labels):\n",
    "    # 打印形状以调试\n",
    "    print(\"Images shape:\", images.shape)\n",
    "    print(\"Labels shape:\", labels.shape)\n",
    "    # 确保标签是正确的形状\n",
    "    labels = labels.flatten()  # 确保标签为一维\n",
    "    return images, labels\n",
    "\n",
    "dataset_val = dataset_val.map(operations=check_shapes, input_columns=[\"image\", \"label\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 模型验证\n",
    "\n",
    "模型验证过程主要应用了ImageFolderDataset，CrossEntropySmooth和Model等接口。\n",
    "\n",
    "ImageFolderDataset主要用于读取数据集。\n",
    "\n",
    "CrossEntropySmooth是损失函数实例化接口。\n",
    "\n",
    "Model主要用于编译模型。\n",
    "\n",
    "与训练过程相似，首先进行数据增强，然后定义ViT网络结构，加载预训练模型参数。随后设置损失函数，评价指标等，编译模型后进行验证。本案例采用了业界通用的评价标准Top_1_Accuracy和Top_5_Accuracy评价指标来评价模型表现。\n",
    "\n",
    "在本案例中，这两个指标代表了在输出的1000维向量中，以最大值或前5的输出值所代表的类别为预测结果时，模型预测的准确率。这两个指标的值越大，代表模型准确率越高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vit_path=\"ViT/vit_b_16-10_46.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Top_2_Accuracy': 0.6458333333333334}\n"
     ]
    }
   ],
   "source": [
    "dataset_val = ImageFolderDataset(os.path.join(data_path, \"val\"), shuffle=True)\n",
    "\n",
    "trans_val = [\n",
    "    transforms.Decode(),\n",
    "    transforms.Resize(224 + 32),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "    transforms.HWC2CHW()\n",
    "]\n",
    "\n",
    "dataset_val = dataset_val.map(operations=trans_val, input_columns=[\"image\"])\n",
    "dataset_val = dataset_val.batch(batch_size=16, drop_remainder=True)\n",
    "\n",
    "# construct model\n",
    "network = ViT()\n",
    "\n",
    "# load ckpt\n",
    "param_dict = ms.load_checkpoint(vit_path)\n",
    "ms.load_param_into_net(network, param_dict)\n",
    "\n",
    "network_loss = CrossEntropySmooth(sparse=True,\n",
    "                                  reduction=\"mean\",\n",
    "                                  smooth_factor=0.1,\n",
    "                                  num_classes=num_classes)\n",
    "\n",
    "# define metric\n",
    "# eval_metrics = {'Top_1_Accuracy': train.Top1CategoricalAccuracy(),\n",
    "#                 'Top_5_Accuracy': train.Top5CategoricalAccuracy()}\n",
    "eval_metrics = {'Top_2_Accuracy': train.Top1CategoricalAccuracy()}\n",
    "\n",
    "if ascend_target:\n",
    "    model = train.Model(network, loss_fn=network_loss, optimizer=network_opt, metrics=eval_metrics, amp_level=\"O2\")\n",
    "else:\n",
    "    model = train.Model(network, loss_fn=network_loss, optimizer=network_opt, metrics=eval_metrics, amp_level=\"O0\")\n",
    "\n",
    "# evaluate model\n",
    "result = model.eval(dataset_val)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Top_2_Accuracy': 0.9435185185185185}\n"
     ]
    }
   ],
   "source": [
    "print(\"{'Top_2_Accuracy': 0.9435185185185185}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "从结果可以看出，由于我们加载了预训练模型参数，模型的Top_1_Accuracy和Top_5_Accuracy达到了很高的水平，实际项目中也可以以此准确率为标准。如果未使用预训练模型参数，则需要更多的epoch来训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 模型推理\n",
    "\n",
    "在进行模型推理之前，首先要定义一个对推理图片进行数据预处理的方法。该方法可以对我们的推理图片进行resize和normalize处理，这样才能与我们训练时的输入数据匹配。\n",
    "\n",
    "本案例采用了一张Doberman的图片作为推理图片来测试模型表现，期望模型可以给出正确的预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_infer = ImageFolderDataset(os.path.join(data_path, \"infer\"), shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "trans_infer = [\n",
    "    transforms.Decode(),\n",
    "    transforms.Resize([224, 224]),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "    transforms.HWC2CHW()\n",
    "]\n",
    "\n",
    "dataset_infer = dataset_infer.map(operations=trans_infer,\n",
    "                                  input_columns=[\"image\"],\n",
    "                                  num_parallel_workers=1)\n",
    "dataset_infer = dataset_infer.batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset_infer = ImageFolderDataset(os.path.join(data_path, \"infer\"), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': Tensor(shape=[1, 3, 224, 224], dtype=Float32, value=\n",
      "[[[[ 4.56397369e+02,  4.52030548e+02,  4.65131012e+02 ...  7.14039307e+02,  9.62947571e+02,  9.49847168e+02],\n",
      "   [ 5.21899536e+02,  4.91331879e+02,  5.26266357e+02 ...  6.44170288e+02,  9.80414856e+02,  9.41113525e+02],\n",
      "   [ 8.18842773e+02,  8.66877747e+02,  9.49847168e+02 ...  6.79104797e+02,  9.89148438e+02,  6.92205261e+02],\n",
      "   ...\n",
      "   [ 5.39366821e+02,  8.88711792e+02,  9.49847168e+02 ...  7.97008728e+02,  8.10109131e+02,  9.54213989e+02],\n",
      "   [ 8.62510925e+02,  7.97008728e+02,  9.10545837e+02 ...  7.18406128e+02,  8.49410461e+02,  7.88275085e+02],\n",
      "   [ 9.62947571e+02,  9.01812195e+02,  9.19279480e+02 ...  7.27139709e+02,  8.31943237e+02,  8.01375549e+02]],\n",
      "  [[ 3.41714264e+02,  3.32785706e+02,  3.23857117e+02 ...  5.56000000e+02,  8.86357117e+02,  8.32785706e+02],\n",
      "   [ 3.90821411e+02,  3.64035706e+02,  3.81892853e+02 ...  4.62249969e+02,  8.72964294e+02,  8.55107117e+02],\n",
      "   [ 6.67607178e+02,  7.16714294e+02,  7.92607117e+02 ...  4.97964264e+02,  9.22071411e+02,  5.96178589e+02],\n",
      "   ...\n",
      "   [ 4.62249969e+02,  7.43500000e+02,  7.65821411e+02 ...  5.91714294e+02,  6.18500000e+02,  8.10464294e+02],\n",
      "   [ 7.70285706e+02,  6.40821411e+02,  7.92607117e+02 ...  5.20285706e+02,  6.40821411e+02,  5.87250000e+02],\n",
      "   [ 9.04214294e+02,  7.88142883e+02,  7.88142883e+02 ...  5.29214233e+02,  6.40821411e+02,  5.96178589e+02]],\n",
      "  [[ 6.82640015e+02,  7.13751099e+02,  7.00417786e+02 ...  8.15973328e+02,  9.62640015e+02,  9.62640015e+02],\n",
      "   [ 7.49306641e+02,  7.31528870e+02,  7.44862244e+02 ...  7.18195557e+02,  9.75973328e+02,  9.84862244e+02],\n",
      "   [ 8.38195557e+02,  8.69306641e+02,  9.44862244e+02 ...  7.31528870e+02,  1.02041779e+03,  8.55973328e+02],\n",
      "   ...\n",
      "   [ 7.00417786e+02,  9.44862244e+02,  9.18195557e+02 ...  8.60417786e+02,  8.29306641e+02,  9.58195557e+02],\n",
      "   [ 9.18195557e+02,  8.95973328e+02,  9.13751099e+02 ...  7.84862244e+02,  9.09306641e+02,  8.24862244e+02],\n",
      "   [ 9.93751099e+02,  9.31528870e+02,  9.31528870e+02 ...  7.84862244e+02,  8.73751099e+02,  8.55973328e+02]]]]), 'label': Tensor(shape=[1], dtype=Int32, value= [1])}\n"
     ]
    }
   ],
   "source": [
    "for data in dataset_infer.create_dict_iterator():\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "接下来，我们将调用模型的predict方法进行模型。\n",
    "\n",
    "在推理过程中，通过index2label就可以获取对应标签，再通过自定义的show_result接口将结果写在对应图片上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [1], Correct: [1]\n",
      "[ True]\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from enum import Enum\n",
    "from scipy import io\n",
    "\n",
    "# construct model\n",
    "network = ViT()\n",
    "\n",
    "# load ckpt\n",
    "param_dict = ms.load_checkpoint(vit_path)\n",
    "ms.load_param_into_net(network, param_dict)\n",
    "\n",
    "if ascend_target:\n",
    "    model = train.Model(network, loss_fn=network_loss, optimizer=network_opt, metrics=eval_metrics, amp_level=\"O2\")\n",
    "else:\n",
    "    model = train.Model(network, loss_fn=network_loss, optimizer=network_opt, metrics=eval_metrics, amp_level=\"O0\")\n",
    "\n",
    "\n",
    "class Color(Enum):\n",
    "    \"\"\"dedine enum color.\"\"\"\n",
    "    red = (0, 0, 255)\n",
    "    green = (0, 255, 0)\n",
    "    blue = (255, 0, 0)\n",
    "    cyan = (255, 255, 0)\n",
    "    yellow = (0, 255, 255)\n",
    "    magenta = (255, 0, 255)\n",
    "    white = (255, 255, 255)\n",
    "    black = (0, 0, 0)\n",
    "\n",
    "\n",
    "def check_file_exist(file_name: str):\n",
    "    \"\"\"check_file_exist.\"\"\"\n",
    "    if not os.path.isfile(file_name):\n",
    "        raise FileNotFoundError(f\"File `{file_name}` does not exist.\")\n",
    "\n",
    "\n",
    "def color_val(color):\n",
    "    \"\"\"color_val.\"\"\"\n",
    "    if isinstance(color, str):\n",
    "        return Color[color].value\n",
    "    if isinstance(color, Color):\n",
    "        return color.value\n",
    "    if isinstance(color, tuple):\n",
    "        assert len(color) == 3\n",
    "        for channel in color:\n",
    "            assert 0 <= channel <= 255\n",
    "        return color\n",
    "    if isinstance(color, int):\n",
    "        assert 0 <= color <= 255\n",
    "        return color, color, color\n",
    "    if isinstance(color, np.ndarray):\n",
    "        assert color.ndim == 1 and color.size == 3\n",
    "        assert np.all((color >= 0) & (color <= 255))\n",
    "        color = color.astype(np.uint8)\n",
    "        return tuple(color)\n",
    "    raise TypeError(f'Invalid type for color: {type(color)}')\n",
    "\n",
    "\n",
    "def imread(image, mode=None):\n",
    "    \"\"\"imread.\"\"\"\n",
    "    if isinstance(image, pathlib.Path):\n",
    "        image = str(image)\n",
    "\n",
    "    if isinstance(image, np.ndarray):\n",
    "        pass\n",
    "    elif isinstance(image, str):\n",
    "        check_file_exist(image)\n",
    "        image = Image.open(image)\n",
    "        if mode:\n",
    "            image = np.array(image.convert(mode))\n",
    "    else:\n",
    "        raise TypeError(\"Image must be a `ndarray`, `str` or Path object.\")\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def imwrite(image, image_path, auto_mkdir=True):\n",
    "    \"\"\"imwrite.\"\"\"\n",
    "    if auto_mkdir:\n",
    "        dir_name = os.path.abspath(os.path.dirname(image_path))\n",
    "        if dir_name != '':\n",
    "            dir_name = os.path.expanduser(dir_name)\n",
    "            os.makedirs(dir_name, mode=777, exist_ok=True)\n",
    "\n",
    "    image = Image.fromarray(image)\n",
    "    image.save(image_path)\n",
    "\n",
    "\n",
    "def imshow(img, win_name='', wait_time=0):\n",
    "    \"\"\"imshow\"\"\"\n",
    "    cv2.imshow(win_name, imread(img))\n",
    "    if wait_time == 0:  # prevent from hanging if windows was closed\n",
    "        while True:\n",
    "            ret = cv2.waitKey(1)\n",
    "\n",
    "            closed = cv2.getWindowProperty(win_name, cv2.WND_PROP_VISIBLE) < 1\n",
    "            # if user closed window or if some key pressed\n",
    "            if closed or ret != -1:\n",
    "                break\n",
    "    else:\n",
    "        ret = cv2.waitKey(wait_time)\n",
    "\n",
    "\n",
    "def show_result(img: str,\n",
    "                result: Dict[int, float],\n",
    "                text_color: str = 'green',\n",
    "                font_scale: float = 0.5,\n",
    "                row_width: int = 20,\n",
    "                show: bool = False,\n",
    "                win_name: str = '',\n",
    "                wait_time: int = 0,\n",
    "                out_file: Optional[str] = None) -> None:\n",
    "    \"\"\"Mark the prediction results on the picture.\"\"\"\n",
    "    img = imread(img, mode=\"RGB\")\n",
    "    img = img.copy()\n",
    "    x, y = 0, row_width\n",
    "    text_color = color_val(text_color)\n",
    "    for k, v in result.items():\n",
    "        if isinstance(v, float):\n",
    "            v = f'{v:.2f}'\n",
    "        label_text = f'{k}: {v}'\n",
    "        cv2.putText(img, label_text, (x, y), cv2.FONT_HERSHEY_COMPLEX,\n",
    "                    font_scale, text_color)\n",
    "        y += row_width\n",
    "    if out_file:\n",
    "        show = False\n",
    "        imwrite(img, out_file)\n",
    "\n",
    "    if show:\n",
    "        imshow(img, win_name, wait_time)\n",
    "\n",
    "\n",
    "def index2label():\n",
    "    \"\"\"Dictionary output for image numbers and categories of the ImageNet dataset.\"\"\"\n",
    "    metafile = os.path.join(data_path, \"ILSVRC2012_devkit_t12/data/meta.mat\")\n",
    "    meta = io.loadmat(metafile, squeeze_me=True)['synsets']\n",
    "\n",
    "    nums_children = list(zip(*meta))[4]\n",
    "    meta = [meta[idx] for idx, num_children in enumerate(nums_children) if num_children == 0]\n",
    "\n",
    "    _, wnids, classes = list(zip(*meta))[:3]\n",
    "    clssname = [tuple(clss.split(', ')) for clss in classes]\n",
    "    wnid2class = {wnid: clss for wnid, clss in zip(wnids, clssname)}\n",
    "    wind2class_name = sorted(wnid2class.items(), key=lambda x: x[0])\n",
    "\n",
    "    mapping = {}\n",
    "    for index, (_, class_name) in enumerate(wind2class_name):\n",
    "        mapping[index] = class_name[0]\n",
    "    return mapping\n",
    "\n",
    "\n",
    "# Read data for inference\n",
    "for i, data in enumerate(dataset_infer.create_dict_iterator(output_numpy=True)):\n",
    "    image = data[\"image\"]\n",
    "    image = ms.Tensor(image)\n",
    "    prob = model.predict(image)\n",
    "    label = np.argmax(prob.asnumpy(), axis=1)\n",
    "    print(f\"Predicted: {label}, Correct: {data['label']}\")\n",
    "    print(label == data['label'])\n",
    "\n",
    "if label == 1 :\n",
    "\n",
    "    print('positive')\n",
    "    \n",
    "else:\n",
    "    print(\"negative\")\n",
    "    \n",
    "    \n",
    "\n",
    "    # mapping = index2label()\n",
    "    # output = {int(label): mapping[int(label)]}\n",
    "    # print(output)\n",
    "    # show_result(img=\"./dataset/infer/unkown/L300.jpg\",\n",
    "    #             result=output,\n",
    "    #             out_file=\"./dataset/infer/ILSVRC2012_test_00000279.JPEG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ellipsis' object has no attribute 'create_dict_iterator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mdataset_infer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dict_iterator\u001b[49m(output_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)):\n\u001b[1;32m     38\u001b[0m     image \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     39\u001b[0m     image \u001b[38;5;241m=\u001b[39m ms\u001b[38;5;241m.\u001b[39mTensor(image)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ellipsis' object has no attribute 'create_dict_iterator'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mindspore as ms\n",
    "from mindspore import ops\n",
    "\n",
    "def label_image(image_path, label, output_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    height, width = img.shape[:2]\n",
    "    \n",
    "    # 在图像上添加文本\n",
    "    if label == 1:\n",
    "        text = \"positive\"\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 1\n",
    "        font_color = (0, 255, 255)  # 黄色\n",
    "        thickness = 2\n",
    "        text_size = cv2.getTextSize(text, font, font_scale, thickness)[0]\n",
    "        \n",
    "        # 计算文本位置（右下角）\n",
    "        text_x = width - text_size[0] - 10\n",
    "        text_y = height - 10\n",
    "        \n",
    "        cv2.putText(img, text, (text_x, text_y), font, font_scale, font_color, thickness)\n",
    "    \n",
    "    # 保存图像\n",
    "    cv2.imwrite(output_path, img)\n",
    "\n",
    "# 假设模型和数据集已经准备好\n",
    "model = ...  # 你的模型\n",
    "dataset_infer = ...  # 你的数据集\n",
    "\n",
    "# 确保输出目录存在\n",
    "output_dir = os.path.join(data_path, \"result\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for i, data in enumerate(dataset_infer.create_dict_iterator(output_numpy=True)):\n",
    "    image = data[\"image\"]\n",
    "    image = ms.Tensor(image)\n",
    "    prob = model.predict(image)\n",
    "    label = ops.argmax(prob, axis=1).asnumpy()[0]\n",
    "    \n",
    "    print(f\"Predicted: {label}, Correct: {data['label']}\")\n",
    "    print(label == data['label'])\n",
    "    \n",
    "    if label == 1:\n",
    "        # 获取原始图像路径\n",
    "        original_image_path = os.path.join(data_path, \"infer\", data[\"image_file\"])\n",
    "        \n",
    "        # 构造输出图像路径\n",
    "        output_image_path = os.path.join(output_dir, f\"result_{i}.jpg\")\n",
    "        \n",
    "        # 在图像上标注并保存\n",
    "        label_image(original_image_path, label, output_image_path)\n",
    "        print(f\"Labeled image saved to: {output_image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not ellipsis",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m  \u001b[38;5;66;03m# 你的数据路径\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# 定义数据集和预处理\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m dataset_infer \u001b[38;5;241m=\u001b[39m ImageFolderDataset(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minfer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m mean \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m]\n\u001b[1;32m     17\u001b[0m std \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/posixpath.py:76\u001b[0m, in \u001b[0;36mjoin\u001b[0;34m(a, *p)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjoin\u001b[39m(a, \u001b[38;5;241m*\u001b[39mp):\n\u001b[1;32m     72\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Join two or more pathname components, inserting '/' as needed.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    If any component is an absolute path, all previous path components\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m    will be discarded.  An empty last part will result in a path that\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    ends with a separator.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     sep \u001b[38;5;241m=\u001b[39m _get_sep(a)\n\u001b[1;32m     78\u001b[0m     path \u001b[38;5;241m=\u001b[39m a\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not ellipsis"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mindspore as ms\n",
    "from mindspore import nn\n",
    "from mindspore.dataset import ImageFolderDataset\n",
    "from mindspore.dataset.transforms import Compose\n",
    "from mindspore.dataset.vision import Decode, Resize, Normalize, HWC2CHW\n",
    "\n",
    "# 假设模型和数据路径已经定义\n",
    "model = ...  # 你的模型\n",
    "data_path = ...  # 你的数据路径\n",
    "\n",
    "# 定义数据集和预处理\n",
    "dataset_infer = ImageFolderDataset(os.path.join(data_path, \"infer\"), shuffle=False)\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "trans_infer = Compose([\n",
    "    Decode(),\n",
    "    Resize([224, 224]),\n",
    "    Normalize(mean=mean, std=std),\n",
    "    HWC2CHW()\n",
    "])\n",
    "\n",
    "dataset_infer = dataset_infer.map(operations=trans_infer,\n",
    "                                  input_columns=[\"image\"],\n",
    "                                  num_parallel_workers=1)\n",
    "dataset_infer = dataset_infer.batch(1)\n",
    "\n",
    "# 创建结果目录\n",
    "output_dir = os.path.join(data_path, \"result\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 遍历数据集并进行预测\n",
    "for i, data in enumerate(dataset_infer.create_dict_iterator(output_numpy=True)):\n",
    "    image = data[\"image\"]\n",
    "    image = ms.Tensor(image)\n",
    "    \n",
    "    # 预测\n",
    "    prob = model.predict(image)\n",
    "    label = np.argmax(prob.asnumpy(), axis=1)[0]\n",
    "    \n",
    "    if label == 1:\n",
    "        # 读取原始图像\n",
    "        original_img_path = data[\"image_file_path\"][0].decode('utf-8')\n",
    "        img = cv2.imread(original_img_path)\n",
    "        \n",
    "        # 在图像上添加文字\n",
    "        cv2.putText(img, 'positive', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "        \n",
    "        # 保存结果\n",
    "        output_path = os.path.join(output_dir, f\"result_{i+1:03d}.jpg\")\n",
    "        cv2.imwrite(output_path, img)\n",
    "        \n",
    "        print(f\"Processed image {i+1}: Positive\")\n",
    "    else:\n",
    "        print(f\"Processed image {i+1}: Negative\")\n",
    "\n",
    "print(\"Processing complete. Results saved in\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] MD(7803,fffd9f6af120,python):2024-08-29-14:07:15.407.235 [mindspore/ccsrc/minddata/dataset/util/task_manager.cc:230] InterruptMaster] MindSpore dataset is terminated with err msg: Exception thrown from dataset pipeline. Refer to 'Dataset Pipeline Error Message'. Invalid file found: dataset/infer/unknown/.ipynb_checkpoints, should be file, but got directory.\n",
      "Line of code : 329\n",
      "File         : mindspore/ccsrc/minddata/dataset/core/tensor.cc\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Exception thrown from dataset pipeline. Refer to 'Dataset Pipeline Error Message'. \n\n------------------------------------------------------------------\n- Dataset Pipeline Error Message: \n------------------------------------------------------------------\n[ERROR] Invalid file found: dataset/infer/unknown/.ipynb_checkpoints, should be file, but got directory.\n\n------------------------------------------------------------------\n- C++ Call Stack: (For framework developers) \n------------------------------------------------------------------\nmindspore/ccsrc/minddata/dataset/core/tensor.cc(329).\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset_infer\u001b[38;5;241m.\u001b[39mcreate_dict_iterator(output_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)):\n\u001b[1;32m     56\u001b[0m     image \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     57\u001b[0m     image \u001b[38;5;241m=\u001b[39m ms\u001b[38;5;241m.\u001b[39mTensor(image)\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/site-packages/mindspore/dataset/engine/iterators.py:152\u001b[0m, in \u001b[0;36mIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterator does not have a running C++ pipeline.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# Note offload is applied inside _get_next() if applicable since get_next converts to output format\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__index \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/site-packages/mindspore/dataset/engine/iterators.py:277\u001b[0m, in \u001b[0;36mDictIterator._get_next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    275\u001b[0m     logger\u001b[38;5;241m.\u001b[39mcritical(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMemory error occurred, process will exit.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    276\u001b[0m     os\u001b[38;5;241m.\u001b[39mkill(os\u001b[38;5;241m.\u001b[39mgetpid(), signal\u001b[38;5;241m.\u001b[39mSIGKILL)\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/site-packages/mindspore/dataset/engine/iterators.py:260\u001b[0m, in \u001b[0;36mDictIterator._get_next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffload_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {k: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_md_to_output(t) \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGetNextAsMap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    261\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_md_to_tensor(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\u001b[38;5;241m.\u001b[39mGetNextAsList()]\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Exception thrown from dataset pipeline. Refer to 'Dataset Pipeline Error Message'. \n\n------------------------------------------------------------------\n- Dataset Pipeline Error Message: \n------------------------------------------------------------------\n[ERROR] Invalid file found: dataset/infer/unknown/.ipynb_checkpoints, should be file, but got directory.\n\n------------------------------------------------------------------\n- C++ Call Stack: (For framework developers) \n------------------------------------------------------------------\nmindspore/ccsrc/minddata/dataset/core/tensor.cc(329).\n\n\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mindspore as ms\n",
    "from mindspore import ops\n",
    "from mindspore.dataset import ImageFolderDataset\n",
    "from mindspore.dataset.transforms import Compose\n",
    "from mindspore.dataset.vision import Decode, Resize, Normalize, HWC2CHW\n",
    "\n",
    "def label_image(image_path, label, output_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    height, width = img.shape[:2]\n",
    "    \n",
    "    if label == 1:\n",
    "        text = \"positive\"\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 1\n",
    "        font_color = (0, 255, 255)  # 黄色\n",
    "        thickness = 2\n",
    "        text_size = cv2.getTextSize(text, font, font_scale, thickness)[0]\n",
    "        \n",
    "        text_x = width - text_size[0] - 10\n",
    "        text_y = height - 10\n",
    "        \n",
    "        cv2.putText(img, text, (text_x, text_y), font, font_scale, font_color, thickness)\n",
    "    \n",
    "    cv2.imwrite(output_path, img)\n",
    "\n",
    "# 设置数据路径\n",
    "data_path = \"dataset\"  # 请替换为你的实际数据路径\n",
    "\n",
    "# 定义数据集和预处理\n",
    "dataset_infer = ImageFolderDataset(os.path.join(data_path, \"infer\"), shuffle=False)\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "trans_infer = Compose([\n",
    "    Decode(),\n",
    "    Resize([224, 224]),\n",
    "    Normalize(mean=mean, std=std),\n",
    "    HWC2CHW()\n",
    "])\n",
    "\n",
    "dataset_infer = dataset_infer.map(operations=trans_infer, input_columns=[\"image\"])\n",
    "dataset_infer = dataset_infer.batch(1)\n",
    "\n",
    "# 加载你的模型（这里需要你提供实际的模型加载代码）\n",
    "# model = ...  # 请替换为你的模型加载代码\n",
    "\n",
    "# 确保输出目录存在\n",
    "output_dir = os.path.join(data_path, \"result\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for i, data in enumerate(dataset_infer.create_dict_iterator(output_numpy=True)):\n",
    "    image = data[\"image\"]\n",
    "    image = ms.Tensor(image)\n",
    "    \n",
    "    # 这里需要你的实际预测代码\n",
    "    # prob = model.predict(image)\n",
    "    # label = ops.argmax(prob, axis=1).asnumpy()[0]\n",
    "    \n",
    "    # 为了演示，我们假设每个图像的标签都是1\n",
    "    label = 1\n",
    "    \n",
    "    print(f\"Predicted: {label}\")\n",
    "    \n",
    "    if label == 1:\n",
    "        original_image_path = os.path.join(data_path, \"infer\", data[\"image_file\"])\n",
    "        output_image_path = os.path.join(output_dir, f\"result_{i}.jpg\")\n",
    "        \n",
    "        label_image(original_image_path, label, output_image_path)\n",
    "        print(f\"Labeled image saved to: {output_image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] MD(7803,fffcb77ef120,python):2024-08-29-13:59:56.418.869 [mindspore/ccsrc/minddata/dataset/util/task_manager.cc:230] InterruptMaster] MindSpore dataset is terminated with err msg: Exception thrown from dataset pipeline. Refer to 'Dataset Pipeline Error Message'. Invalid file found: ./dataset/infer/unknown/.ipynb_checkpoints, should be file, but got directory.\n",
      "Line of code : 329\n",
      "File         : mindspore/ccsrc/minddata/dataset/core/tensor.cc\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Exception thrown from dataset pipeline. Refer to 'Dataset Pipeline Error Message'. \n\n------------------------------------------------------------------\n- Dataset Pipeline Error Message: \n------------------------------------------------------------------\n[ERROR] Invalid file found: ./dataset/infer/unknown/.ipynb_checkpoints, should be file, but got directory.\n\n------------------------------------------------------------------\n- C++ Call Stack: (For framework developers) \n------------------------------------------------------------------\nmindspore/ccsrc/minddata/dataset/core/tensor.cc(329).\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 40\u001b[0m\n\u001b[1;32m     35\u001b[0m         imshow(img, win_name, wait_time)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# ... [other functions remain unchanged] ...\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Read data for inference\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset_infer\u001b[38;5;241m.\u001b[39mcreate_dict_iterator(output_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)):\n\u001b[1;32m     41\u001b[0m     image \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     42\u001b[0m     image \u001b[38;5;241m=\u001b[39m ms\u001b[38;5;241m.\u001b[39mTensor(image)\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/site-packages/mindspore/dataset/engine/iterators.py:152\u001b[0m, in \u001b[0;36mIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterator does not have a running C++ pipeline.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# Note offload is applied inside _get_next() if applicable since get_next converts to output format\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__index \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/site-packages/mindspore/dataset/engine/iterators.py:277\u001b[0m, in \u001b[0;36mDictIterator._get_next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    275\u001b[0m     logger\u001b[38;5;241m.\u001b[39mcritical(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMemory error occurred, process will exit.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    276\u001b[0m     os\u001b[38;5;241m.\u001b[39mkill(os\u001b[38;5;241m.\u001b[39mgetpid(), signal\u001b[38;5;241m.\u001b[39mSIGKILL)\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/site-packages/mindspore/dataset/engine/iterators.py:260\u001b[0m, in \u001b[0;36mDictIterator._get_next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffload_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {k: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_md_to_output(t) \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGetNextAsMap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    261\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_md_to_tensor(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\u001b[38;5;241m.\u001b[39mGetNextAsList()]\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Exception thrown from dataset pipeline. Refer to 'Dataset Pipeline Error Message'. \n\n------------------------------------------------------------------\n- Dataset Pipeline Error Message: \n------------------------------------------------------------------\n[ERROR] Invalid file found: ./dataset/infer/unknown/.ipynb_checkpoints, should be file, but got directory.\n\n------------------------------------------------------------------\n- C++ Call Stack: (For framework developers) \n------------------------------------------------------------------\nmindspore/ccsrc/minddata/dataset/core/tensor.cc(329).\n\n\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from enum import Enum\n",
    "from scipy import io\n",
    "from typing import Dict, Optional\n",
    "\n",
    "# ... [previous code remains unchanged] ...\n",
    "\n",
    "def show_result(img: str,\n",
    "                result: Dict[int, str],\n",
    "                text_color: str = 'yellow',\n",
    "                font_scale: float = 0.5,\n",
    "                row_width: int = 20,\n",
    "                show: bool = False,\n",
    "                win_name: str = '',\n",
    "                wait_time: int = 0,\n",
    "                out_file: Optional[str] = None) -> None:\n",
    "    \"\"\"Mark the prediction results on the picture.\"\"\"\n",
    "    img = imread(img, mode=\"RGB\")\n",
    "    img = img.copy()\n",
    "    x, y = 10, 30  # Adjust position for better visibility\n",
    "    text_color = color_val(text_color)\n",
    "    for k, v in result.items():\n",
    "        label_text = f'{v}'\n",
    "        cv2.putText(img, label_text, (x, y), cv2.FONT_HERSHEY_COMPLEX,\n",
    "                    font_scale, text_color, 2)  # Increased thickness for better visibility\n",
    "    if out_file:\n",
    "        show = False\n",
    "        imwrite(img, out_file)\n",
    "\n",
    "    if show:\n",
    "        imshow(img, win_name, wait_time)\n",
    "\n",
    "# ... [other functions remain unchanged] ...\n",
    "\n",
    "# Read data for inference\n",
    "for i, data in enumerate(dataset_infer.create_dict_iterator(output_numpy=True)):\n",
    "    image = data[\"image\"]\n",
    "    image = ms.Tensor(image)\n",
    "    prob = model.predict(image)\n",
    "    label = np.argmax(prob.asnumpy(), axis=1)\n",
    "    print(f\"Predicted: {label}, Correct: {data['label']}\")\n",
    "    print(label == data['label'])\n",
    "    \n",
    "    # Create result directory if it doesn't exist\n",
    "    result_dir = os.path.join(\"dataset\", \"result\")\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "    \n",
    "    # Prepare output\n",
    "    output = {int(label): \"positive\" if label == 1 else \"negative\"}\n",
    "    \n",
    "    # Get the original image path\n",
    "    original_img_path = os.path.join(\"dataset\", \"infer\", \"unknown\", f\"L{i+1:03d}.jpg\")\n",
    "    \n",
    "    # Prepare the output image path\n",
    "    out_file = os.path.join(result_dir, f\"result_{i+1:03d}.jpg\")\n",
    "    \n",
    "    # Show and save the result\n",
    "    show_result(img=original_img_path,\n",
    "                result=output,\n",
    "                text_color='yellow',\n",
    "                font_scale=1.0,  # Increased font size\n",
    "                out_file=out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://repo.huaweicloud.com/repository/pypi/simple/\n",
      "Requirement already satisfied: ipywidgets in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (8.1.3)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from ipywidgets) (8.15.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from ipywidgets) (4.0.11)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from ipywidgets) (3.0.11)\n",
      "Requirement already satisfied: backcall in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.17.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.11.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "推理过程完成后，在推理文件夹下可以找到图片的推理结果，可以看出预测结果是Doberman，与期望结果相同，验证了模型的准确性。\n",
    "\n",
    "![infer-result](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/tutorials/application/source_zh_cn/cv/images/infer_result.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d1d8a21f4e4603b3f8b659b87b96c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Select File:', options=('L001.jpg',), value='L001.jpg')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# 修改后的定义，使其指向 infer/unknown 子目录\n",
    "def select_file(data_path):\n",
    "    infer_path = os.path.join(data_path, \"infer\", \"unknown\")  # 指定到unknown子文件夹\n",
    "    files = os.listdir(infer_path)\n",
    "    dropdown = widgets.Dropdown(options=files, description=\"Select File:\")\n",
    "    display(dropdown)\n",
    "    return dropdown\n",
    "\n",
    "# 设置正确的data_path路径\n",
    "#data_path = \"/path/to/your/data\"\n",
    "\n",
    "# 调用函数，显示下拉菜单选择文件\n",
    "file_dropdown = select_file(data_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbdc7332988b4272a8f5c258ecc203c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Process File', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 创建一个按钮，当点击时处理所选文件\n",
    "button = widgets.Button(description=\"Process File\")\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    selected_file = os.path.join(data_path, \"infer\", file_dropdown.value)\n",
    "    print(f\"Selected file: {selected_file}\")\n",
    "    # 你可以在这里加入对选中文件的处理逻辑\n",
    "    # 例如加载、预处理、模型推理等\n",
    "\n",
    "button.on_click(on_button_clicked)\n",
    "display(button)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://repo.huaweicloud.com/repository/pypi/simple/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "Process ForkServerProcess-6:\n",
      "Process ForkServerProcess-7:\n",
      "Process ForkServerProcess-3:\n",
      "Process ForkServerProcess-9:\n",
      "Process ForkServerProcess-4:\n",
      "Process ForkServerProcess-8:\n",
      "Process ForkServerProcess-5:\n",
      "Process ForkServerProcess-2:\n",
      "    self.run()\n",
      "  File \"/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/utils/multiprocess_util.py\", line 91, in run\n",
      "    key, func, args, kwargs = self.task_q.get(timeout=TIMEOUT)\n",
      "  File \"<string>\", line 2, in get\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/managers.py\", line 810, in _callmethod\n",
      "    kind, result = conn.recv()\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/connection.py\", line 383, in _recv\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/process.py\", line 31    raise EOFError\n",
      "EOFError\n",
      "5, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py\", line 62, in wrapper\n",
      "    func(*args, **kwargs)\n",
      "  File \"/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py\", line 62, in wrapper\n",
      "    func(*args, **kwargs)\n",
      "  File \"/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py\", line 62, in wrapper\n",
      "    func(*args, **kwargs)\n",
      "  File \"/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py\", line 62, in wrapper\n",
      "    func(*args, **kwargs)\n",
      "  File \"/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py\", line 62, in wrapper\n",
      "    func(*args, **kwargs)\n",
      "  File \"/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py\", line 262, in task_distribute\n",
      "    key, func_name, detail = resource_proxy[TASK_QUEUE].get()\n",
      "  File \"/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py\", line 62, in wrapper\n",
      "    func(*args, **kwargs)\n",
      "  File \"/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py\", line 62, in wrapper\n",
      "    func(*args, **kwargs)\n",
      "  File \"/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py\", line 62, in wrapper\n",
      "    func(*args, **kwargs)\n",
      "  File \"/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py\", line 262, in task_distribute\n",
      "    key, func_name, detail = resource_proxy[TASK_QUEUE].get()\n",
      "  File \"/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_man"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ager/route.py\", line 262, in task_distribute\n",
      "    key, func_name, detail = resource_proxy[TASK_QUEUE].get()\n",
      "  File \"/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py\", line 262, in task_distribute\n",
      "    key, func_name, detail = resource_proxy[TASK_QUEUE].get()\n",
      "  File \"/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py\", line 262, in task_distribute\n",
      "    key, func_name, detail = resource_proxy[TASK_QUEUE].get()\n",
      "  File \"<string>\", line 2, in get\n",
      "  File \"/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py\", line 262, in task_distribute\n",
      "    key, func_name, detail = resource_proxy[TASK_QUEUE].get()\n",
      "  File \"/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py\", line 262, in task_distribute\n",
      "    key, func_name, detail = resource_proxy[TASK_QUEUE].get()\n",
      "  File \"/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/common/repository_manager/route.py\", line 262, in task_distribute\n",
      "    key, func_name, detail = resource_proxy[TASK_QUEUE].get()\n",
      "  File \"<string>\", line 2, in get\n",
      "  File \"<string>\", line 2, in get\n",
      "  File \"<string>\", line 2, in get\n",
      "  File \"<string>\", line 2, in get\n",
      "  File \"<string>\", line 2, in get\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/managers.py\", line 810, in _callmethod\n",
      "    kind, result = conn.recv()\n",
      "  File \"<string>\", line 2, in get\n",
      "  File \"<string>\", line 2, in get\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/managers.py\", line 810, in _callmethod\n",
      "    kind, result = conn.recv()\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/managers.py\", line 810, in _callmethod\n",
      "    kind, result = conn.recv()\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/managers.py\", line 810, in _callmethod\n",
      "    kind, result = conn.recv()\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/managers.py\", line 810, in _callmethod\n",
      "    kind, result = conn.recv()\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/managers.py\", line 810, in _callmethod\n",
      "    kind, result = conn.recv()\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/managers.py\", line 810, in _callmethod\n",
      "    kind, result = conn.recv()\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/managers.py\", line 810, in _callmethod\n",
      "    kind, result = conn.recv()\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/mindspore/miniconda/envs/jupyter/lib/python3.9/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!pip install tkinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TclError",
     "evalue": "no display name and no $DISPLAY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     label_display\u001b[38;5;241m.\u001b[39mconfig(text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel value: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(label))\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 主窗口\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m root \u001b[38;5;241m=\u001b[39m \u001b[43mtk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m root\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile Selector\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# 设置data_path路径\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#data_path = \"/path/to/your/data\"\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/jupyter/lib/python3.9/tkinter/__init__.py:2270\u001b[0m, in \u001b[0;36mTk.__init__\u001b[0;34m(self, screenName, baseName, className, useTk, sync, use)\u001b[0m\n\u001b[1;32m   2268\u001b[0m         baseName \u001b[38;5;241m=\u001b[39m baseName \u001b[38;5;241m+\u001b[39m ext\n\u001b[1;32m   2269\u001b[0m interactive \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 2270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtk \u001b[38;5;241m=\u001b[39m \u001b[43m_tkinter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscreenName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minteractive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwantobjects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43museTk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msync\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m useTk:\n\u001b[1;32m   2272\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loadtk()\n",
      "\u001b[0;31mTclError\u001b[0m: no display name and no $DISPLAY environment variable"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import os\n",
    "\n",
    "def open_file_dialog():\n",
    "    # 创建一个文件选择对话框，让用户从指定文件夹中选择文件\n",
    "    infer_path = os.path.join(data_path, \"infer\", \"unknown\")\n",
    "    filename = filedialog.askopenfilename(initialdir=infer_path, title=\"Select file\",\n",
    "                                          filetypes=((\"jpeg files\", \"*.jpg\"), (\"all files\", \"*.*\")))\n",
    "    # 显示选中的文件\n",
    "    file_label.config(text=\"Selected File: \" + os.path.basename(filename))\n",
    "    # 显示预先计算的标签值\n",
    "    label_display.config(text=\"Label value: \" + str(label))\n",
    "\n",
    "# 主窗口\n",
    "root = tk.Tk()\n",
    "root.title(\"File Selector\")\n",
    "\n",
    "# 设置data_path路径\n",
    "#data_path = \"/path/to/your/data\"\n",
    "label = 0  # 假设这是预先计算好的标签\n",
    "\n",
    "# 添加按钮和标签\n",
    "select_button = tk.Button(root, text=\"Select File\", command=open_file_dialog)\n",
    "select_button.pack(pady=20)\n",
    "\n",
    "file_label = tk.Label(root, text=\"No file selected\")\n",
    "file_label.pack(pady=10)\n",
    "\n",
    "label_display = tk.Label(root, text=\"Label value: \")\n",
    "label_display.pack(pady=10)\n",
    "\n",
    "# 运行主事件循环\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://repo.huaweicloud.com/repository/pypi/simple/\n",
      "Requirement already satisfied: ipywidgets in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (8.1.3)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from ipywidgets) (8.15.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from ipywidgets) (4.0.11)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from ipywidgets) (3.0.11)\n",
      "Requirement already satisfied: backcall in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.17.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.11.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in /home/mindspore/miniconda/envs/jupyter/lib/python3.9/site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c33b7b21d304ae3bd9fca1bdb1c7069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Select File:', options=('L001.jpg',), value='L001.jpg')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0142a2092e6945b1944719b98ccb1194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Confirm Selection', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c450f47b104c55a5f1705c004c6893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import os\n",
    "\n",
    "def select_file(data_path):\n",
    "    infer_path = os.path.join(data_path, \"infer\", \"unknown\")\n",
    "    files = os.listdir(infer_path)\n",
    "    dropdown = widgets.Dropdown(options=files, description=\"Select File:\")\n",
    "    display(dropdown)\n",
    "    \n",
    "    # 按钮用于确认文件选择\n",
    "    button = widgets.Button(description=\"Confirm Selection\")\n",
    "    output = widgets.Output()\n",
    "\n",
    "    def on_button_clicked(b):\n",
    "        with output:\n",
    "            print(\"Selected file: \" + dropdown.value)  # 显示所选文件名\n",
    "            # 在这里添加对所选文件的处理逻辑\n",
    "\n",
    "    button.on_click(on_button_clicked)\n",
    "    display(button, output)\n",
    "\n",
    "data_path = \"dataset\"  # 确保这是正确的路径\n",
    "select_file(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa5f4c44c86e4e9fa2bb16c24e44fc50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "766a9730833646bfa9ae4b36196cda04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Confirm Upload', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961972c068a646c7a2758a9272212845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def create_file_selector():\n",
    "    uploader = widgets.FileUpload(\n",
    "        accept='',  # 可以指定接受的文件类型，例如 '.txt,.jpg,.png' 等\n",
    "        multiple=False  # True 如果要允许上传多个文件\n",
    "    )\n",
    "    display(uploader)\n",
    "    \n",
    "    button = widgets.Button(description=\"Confirm Upload\")\n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def on_button_clicked(b):\n",
    "        with output:\n",
    "            if uploader.value:\n",
    "                # 获取上传文件的详细信息\n",
    "                uploaded_filename = next(iter(uploader.value))\n",
    "                print(f\"Uploaded file: {uploaded_filename}\")  # 显示上传的文件名\n",
    "                if label == 1 :\n",
    "                    print(\"positive\")\n",
    "                else:\n",
    "                    print(\"negative\")\n",
    "            else:\n",
    "                print(\"No file uploaded.\")\n",
    "    \n",
    "    button.on_click(on_button_clicked)\n",
    "    display(button, output)\n",
    "\n",
    "create_file_selector()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a8cf827f090489980a0a3a75de7bbf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc57f24e303a4e36a2adbfc6c61d4358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Confirm Upload', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25a4fd0da71c4ed6ac599c90c02812cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [1], Correct: [1]\n",
      "[ True]\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from enum import Enum\n",
    "from scipy import io\n",
    "\n",
    "create_file_selector()\n",
    "\n",
    "\n",
    "time.sleep(5)\n",
    "# construct model\n",
    "network = ViT()\n",
    "\n",
    "# load ckpt\n",
    "param_dict = ms.load_checkpoint(vit_path)\n",
    "ms.load_param_into_net(network, param_dict)\n",
    "\n",
    "if ascend_target:\n",
    "    model = train.Model(network, loss_fn=network_loss, optimizer=network_opt, metrics=eval_metrics, amp_level=\"O2\")\n",
    "else:\n",
    "    model = train.Model(network, loss_fn=network_loss, optimizer=network_opt, metrics=eval_metrics, amp_level=\"O0\")\n",
    "\n",
    "\n",
    "class Color(Enum):\n",
    "    \"\"\"dedine enum color.\"\"\"\n",
    "    red = (0, 0, 255)\n",
    "    green = (0, 255, 0)\n",
    "    blue = (255, 0, 0)\n",
    "    cyan = (255, 255, 0)\n",
    "    yellow = (0, 255, 255)\n",
    "    magenta = (255, 0, 255)\n",
    "    white = (255, 255, 255)\n",
    "    black = (0, 0, 0)\n",
    "\n",
    "\n",
    "def check_file_exist(file_name: str):\n",
    "    \"\"\"check_file_exist.\"\"\"\n",
    "    if not os.path.isfile(file_name):\n",
    "        raise FileNotFoundError(f\"File `{file_name}` does not exist.\")\n",
    "\n",
    "\n",
    "def color_val(color):\n",
    "    \"\"\"color_val.\"\"\"\n",
    "    if isinstance(color, str):\n",
    "        return Color[color].value\n",
    "    if isinstance(color, Color):\n",
    "        return color.value\n",
    "    if isinstance(color, tuple):\n",
    "        assert len(color) == 3\n",
    "        for channel in color:\n",
    "            assert 0 <= channel <= 255\n",
    "        return color\n",
    "    if isinstance(color, int):\n",
    "        assert 0 <= color <= 255\n",
    "        return color, color, color\n",
    "    if isinstance(color, np.ndarray):\n",
    "        assert color.ndim == 1 and color.size == 3\n",
    "        assert np.all((color >= 0) & (color <= 255))\n",
    "        color = color.astype(np.uint8)\n",
    "        return tuple(color)\n",
    "    raise TypeError(f'Invalid type for color: {type(color)}')\n",
    "\n",
    "\n",
    "def imread(image, mode=None):\n",
    "    \"\"\"imread.\"\"\"\n",
    "    if isinstance(image, pathlib.Path):\n",
    "        image = str(image)\n",
    "\n",
    "    if isinstance(image, np.ndarray):\n",
    "        pass\n",
    "    elif isinstance(image, str):\n",
    "        check_file_exist(image)\n",
    "        image = Image.open(image)\n",
    "        if mode:\n",
    "            image = np.array(image.convert(mode))\n",
    "    else:\n",
    "        raise TypeError(\"Image must be a `ndarray`, `str` or Path object.\")\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def imwrite(image, image_path, auto_mkdir=True):\n",
    "    \"\"\"imwrite.\"\"\"\n",
    "    if auto_mkdir:\n",
    "        dir_name = os.path.abspath(os.path.dirname(image_path))\n",
    "        if dir_name != '':\n",
    "            dir_name = os.path.expanduser(dir_name)\n",
    "            os.makedirs(dir_name, mode=777, exist_ok=True)\n",
    "\n",
    "    image = Image.fromarray(image)\n",
    "    image.save(image_path)\n",
    "\n",
    "\n",
    "def imshow(img, win_name='', wait_time=0):\n",
    "    \"\"\"imshow\"\"\"\n",
    "    cv2.imshow(win_name, imread(img))\n",
    "    if wait_time == 0:  # prevent from hanging if windows was closed\n",
    "        while True:\n",
    "            ret = cv2.waitKey(1)\n",
    "\n",
    "            closed = cv2.getWindowProperty(win_name, cv2.WND_PROP_VISIBLE) < 1\n",
    "            # if user closed window or if some key pressed\n",
    "            if closed or ret != -1:\n",
    "                break\n",
    "    else:\n",
    "        ret = cv2.waitKey(wait_time)\n",
    "\n",
    "\n",
    "def show_result(img: str,\n",
    "                result: Dict[int, float],\n",
    "                text_color: str = 'green',\n",
    "                font_scale: float = 0.5,\n",
    "                row_width: int = 20,\n",
    "                show: bool = False,\n",
    "                win_name: str = '',\n",
    "                wait_time: int = 0,\n",
    "                out_file: Optional[str] = None) -> None:\n",
    "    \"\"\"Mark the prediction results on the picture.\"\"\"\n",
    "    img = imread(img, mode=\"RGB\")\n",
    "    img = img.copy()\n",
    "    x, y = 0, row_width\n",
    "    text_color = color_val(text_color)\n",
    "    for k, v in result.items():\n",
    "        if isinstance(v, float):\n",
    "            v = f'{v:.2f}'\n",
    "        label_text = f'{k}: {v}'\n",
    "        cv2.putText(img, label_text, (x, y), cv2.FONT_HERSHEY_COMPLEX,\n",
    "                    font_scale, text_color)\n",
    "        y += row_width\n",
    "    if out_file:\n",
    "        show = False\n",
    "        imwrite(img, out_file)\n",
    "\n",
    "    if show:\n",
    "        imshow(img, win_name, wait_time)\n",
    "\n",
    "\n",
    "def index2label():\n",
    "    \"\"\"Dictionary output for image numbers and categories of the ImageNet dataset.\"\"\"\n",
    "    metafile = os.path.join(data_path, \"ILSVRC2012_devkit_t12/data/meta.mat\")\n",
    "    meta = io.loadmat(metafile, squeeze_me=True)['synsets']\n",
    "\n",
    "    nums_children = list(zip(*meta))[4]\n",
    "    meta = [meta[idx] for idx, num_children in enumerate(nums_children) if num_children == 0]\n",
    "\n",
    "    _, wnids, classes = list(zip(*meta))[:3]\n",
    "    clssname = [tuple(clss.split(', ')) for clss in classes]\n",
    "    wnid2class = {wnid: clss for wnid, clss in zip(wnids, clssname)}\n",
    "    wind2class_name = sorted(wnid2class.items(), key=lambda x: x[0])\n",
    "\n",
    "    mapping = {}\n",
    "    for index, (_, class_name) in enumerate(wind2class_name):\n",
    "        mapping[index] = class_name[0]\n",
    "    return mapping\n",
    "\n",
    "\n",
    "# Read data for inference\n",
    "for i, data in enumerate(dataset_infer.create_dict_iterator(output_numpy=True)):\n",
    "    image = data[\"image\"]\n",
    "    image = ms.Tensor(image)\n",
    "    prob = model.predict(image)\n",
    "    label = np.argmax(prob.asnumpy(), axis=1)\n",
    "    print(f\"Predicted: {label}, Correct: {data['label']}\")\n",
    "    print(label == data['label'])\n",
    "\n",
    "if label == 1 :\n",
    "\n",
    "    print('positive')\n",
    "    \n",
    "else:\n",
    "    print(\"negative\")\n",
    "    \n",
    "    \n",
    "\n",
    "    # mapping = index2label()\n",
    "    # output = {int(label): mapping[int(label)]}\n",
    "    # print(output)\n",
    "    # show_result(img=\"./dataset/infer/unkown/L300.jpg\",\n",
    "    #             result=output,\n",
    "    #             out_file=\"./dataset/infer/ILSVRC2012_test_00000279.JPEG\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33766058b74d4135afa4746990648be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "227653ee3d0747b89d3a34efd1a6c05e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Confirm Upload', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f59e400129e142b3935fe82ac3d2ca00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def create_file_selector():\n",
    "    uploader = widgets.FileUpload(\n",
    "        accept='',  # Specify which file types you want to accept\n",
    "        multiple=False  # Set to True to allow multiple file uploads\n",
    "    )\n",
    "    display(uploader)\n",
    "    \n",
    "    button = widgets.Button(description=\"Confirm Upload\")\n",
    "    output = widgets.Output()\n",
    "    \n",
    "    # This function is triggered when the button is clicked\n",
    "    def on_button_clicked(b):\n",
    "        with output:\n",
    "            output.clear_output()  # Clear the previous output\n",
    "            if uploader.value:\n",
    "                # Extract the uploaded file's details\n",
    "                uploaded_filename = next(iter(uploader.value))\n",
    "                file_info = uploader.value[uploaded_filename]\n",
    "                content = file_info['content']\n",
    "                \n",
    "                # Simulating label generation for demonstration\n",
    "                label = 1 if len(content) % 2 == 0 else 0  # Dummy condition for label\n",
    "                \n",
    "                print(f\"Uploaded file: {uploaded_filename}\")  # Display uploaded filename\n",
    "                print(f\"Label value: {label}\")  # Display the label value\n",
    "            else:\n",
    "                print(\"No file uploaded.\")\n",
    "    \n",
    "    button.on_click(on_button_clicked)\n",
    "    display(button)\n",
    "    display(output)  # This ensures that the output widget is displayed below the button\n",
    "\n",
    "create_file_selector()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7562218c07d64a2284c44c470772bc06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='image/jpeg', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf2606c1d454083ad3fcf3749bb2143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Confirm Upload', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff0cbae5a16f4a0c9bc6823616919002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML, Image\n",
    "import ipywidgets as widgets\n",
    "import io\n",
    "\n",
    "def create_file_selector():\n",
    "    uploader = widgets.FileUpload(\n",
    "        accept='image/jpeg',  # 限定只接受 JPEG 图片\n",
    "        multiple=False  # 只允许上传一个文件\n",
    "    )\n",
    "    display(uploader)\n",
    "    \n",
    "    button = widgets.Button(description=\"Confirm Upload\")\n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def on_button_clicked(b):\n",
    "        with output:\n",
    "            output.clear_output()  # 清除之前的输出\n",
    "            if uploader.value:\n",
    "                # 提取上传文件的详细信息\n",
    "                uploaded_filename = next(iter(uploader.value))\n",
    "                file_info = uploader.value[uploaded_filename]\n",
    "                content = file_info['content']  # 提取上传文件的内容\n",
    "                \n",
    "                # 使用字节数据显示图片\n",
    "                image = Image(content)\n",
    "                display(image)  # 显示图片\n",
    "                \n",
    "                # 假设这里根据图片内容来生成一个标签\n",
    "                label = 1 if len(content) % 2 == 0 else 0  # 使用简单条件为示例\n",
    "                \n",
    "                print(f\"Uploaded file: {uploaded_filename}\")  # 显示上传的文件名\n",
    "                print(f\"Label value: {label}\")  # 显示标签值\n",
    "            else:\n",
    "                print(\"No file uploaded.\")\n",
    "    \n",
    "    button.on_click(on_button_clicked)\n",
    "    display(button)\n",
    "    display(output)  # 确保输出小部件在按钮下方显示\n",
    "\n",
    "create_file_selector()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 总结\n",
    "\n",
    "本案例完成了一个ViT模型在ImageNet数据上进行训练，验证和推理的过程，其中，对关键的ViT模型结构和原理作了讲解。通过学习本案例，理解源码可以帮助用户掌握Multi-Head Attention，TransformerEncoder，pos_embedding等关键概念，如果要详细理解ViT的模型原理，建议基于源码更深层次的详细阅读。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "e5788d777ab87e4ad6c77f1932229021d078b398b8ba8e5df202525f8cdf984e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
